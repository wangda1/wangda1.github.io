<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="千淘万漉虽辛苦，吹尽黄沙始到金">
<meta property="og:type" content="website">
<meta property="og:title" content="笔记记录">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;16&#x2F;index.html">
<meta property="og:site_name" content="笔记记录">
<meta property="og:description" content="千淘万漉虽辛苦，吹尽黄沙始到金">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/16/"/>





  <title>笔记记录</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">笔记记录</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/RNN/" itemprop="url">MLDL/RNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="RNN-循环神经网络"><a href="#RNN-循环神经网络" class="headerlink" title="RNN(循环神经网络)"></a>RNN(循环神经网络)</h1><p><em>参考：《深度学习入门之PyTorch》配套代码地址：<a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch" target="_blank" rel="noopener">https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch</a></em></p>
<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>卷积神经网络相当于人类的视觉，但是它没有记忆能力，只能处理一种特定的视觉任务，没办法根据以前的记忆来处理新的任务。</p>
<p>循环神经网络的提出是基于记忆模型的想法，期望网络能够记住前面出现的特征，并依据特征推断后面的结果，并且整体的网络结构不断循环，因此得名循环神经网络。</p>
<h2 id="2-网络结构"><a href="#2-网络结构" class="headerlink" title="2. 网络结构"></a>2. 网络结构</h2><p>使神经网络具有记忆性，一个很直观的做法是将隐藏层的信息进行保留作为下一个的输入，由此产生出的有以下几种网络结构：</p>
<h3 id="2-1-基本-RNN"><a href="#2-1-基本-RNN" class="headerlink" title="2.1 基本 RNN"></a>2.1 基本 RNN</h3><p><img src="./IMG/base_rnn.png" alt="base_RNN"></p>
<h3 id="2-2-双向-RNN（BiRNN"><a href="#2-2-双向-RNN（BiRNN" class="headerlink" title="2.2 双向 RNN（BiRNN)"></a>2.2 双向 RNN（BiRNN)</h3><p><img src="./IMG/BiRNN.png" alt="Bi_RNN"></p>
<h3 id="2-3-深层-RNN"><a href="#2-3-深层-RNN" class="headerlink" title="2.3 深层 RNN"></a>2.3 深层 RNN</h3><p><img src="./IMG/deep_RNN.jpg" alt="Deep_RNN"></p>
<p><em>以上图示均为按照时间线展开，实际只是网络的循环，故名循环神经网络</em></p>
<h2 id="3-存在的问题"><a href="#3-存在的问题" class="headerlink" title="3. 存在的问题"></a>3. 存在的问题</h2><p>早期的循环神经网络具有特别好的记忆性，能够记住当前情景下的内容，但这种循环神经网络能够解决短时依赖问题，对于长时依赖问题，由于记忆的信息和预测位置之间的跨度太大，网络并不能记忆这么长时间的信息，而且随着时间跨度的增大，循环神经网络也越来越难学习这些信息。</p>
<h2 id="4-循环神经网络的变式：LSTM-与-GRU"><a href="#4-循环神经网络的变式：LSTM-与-GRU" class="headerlink" title="4. 循环神经网络的变式：LSTM 与 GRU"></a>4. 循环神经网络的变式：LSTM 与 GRU</h2><h3 id="4-1-LSTM"><a href="#4-1-LSTM" class="headerlink" title="4.1 LSTM"></a>4.1 LSTM</h3><p>LSTM 是 Long Short Term Memory Networks的缩写，长短期记忆模型。LSTM 比基本的循环神经网络的网络结构内部要复杂。</p>
<p>LSTM 的三个门：</p>
<ul>
<li>输入门，输入门控制着网络的输入</li>
<li>遗忘门，遗忘门控制着记忆单元</li>
<li>输出门，输出门控制着网络的输出</li>
</ul>
<p><img src="./IMG/LSTM.png" alt="LSTM"></p>
<p><img src="./IMG/LSTM_1.jpg" alt="LSTM"></p>
<p><img src="./IMG/LSTM_2.jpg" alt="LSTM"></p>
<p><img src="./IMG/LSTM_3.jpg" alt="LSTM"></p>
<h3 id="4-2-GRU"><a href="#4-2-GRU" class="headerlink" title="4.2 GRU"></a>4.2 GRU</h3><p>GRU是 Gated Recurrent Unit的缩写，由 cho 在2014年提出。GRU 和 LSTM 最大的不同在于 GRU 将遗忘门和输入门合成了一个 “更新门”， 同时网络只进行传递和保存的状态是 <code>ht</code>.</p>
<p><img src="./IMG/LSTM_4.jpg" alt="LSTM"></p>
<h3 id="4-3-收敛性问题"><a href="#4-3-收敛性问题" class="headerlink" title="4.3 收敛性问题"></a>4.3 收敛性问题</h3><p>RNN 的误差曲面上存在很多陡峭的斜坡，斜坡上误差的变化率特别大，正是这个原因导致道路 loss 曲线在不断跳跃。</p>
<blockquote>
<p>正是因为网络在训练过程中，梯度的变化范围很大，所以设置一个固定的学习率并不能有效收敛，同时梯度的变化并没有规律，所以设置衰减的学习率也不能满足条件。</p>
</blockquote>
<p>可以采用的办法是： <strong>梯度裁剪</strong>，使用梯度裁剪能够将大的梯度裁，在一定程度上避免收敛不好的问题。</p>
<h2 id="5-循环神经网络的-pytorch-实现"><a href="#5-循环神经网络的-pytorch-实现" class="headerlink" title="5. 循环神经网络的 pytorch 实现"></a>5. 循环神经网络的 pytorch 实现</h2><h3 id="5-1-PyTorch-的循环网络模块"><a href="#5-1-PyTorch-的循环网络模块" class="headerlink" title="5.1 PyTorch 的循环网络模块"></a>5.1 PyTorch 的循环网络模块</h3><h4 id="1-标准-RNN"><a href="#1-标准-RNN" class="headerlink" title="1. 标准 RNN"></a>1. 标准 RNN</h4><ul>
<li><code>nn.RNN()</code></li>
</ul>
<p>关于参数设置：</p>
<ul>
<li><code>input_size</code> 表示输入 x 的特征维度</li>
<li><code>hidden_size</code> 表示输出 h 的特征维度</li>
<li><code>num_layers</code> 表示网络层数，默认是 1 层</li>
<li><code>nonlinearity</code> 表示非线性激活函数的选择，默认是 <code>tanh</code>，可以选择 <code>relu</code></li>
<li><code>bias</code> 表示是否使用偏置，默认是 <code>True</code></li>
<li><code>batch_first</code> 决定网络输入的维度顺序，默认网络输入按照 <code>(seq, batch, feature)</code> ，解释为：序列长度、批量、特征维度；如果该参数设置为 <code>True</code>，则顺序变成 <code>(batch, seq, feature)</code></li>
<li><code>dropout</code> 参数接收0-1的数值，会在网络中除了最后一层之外的其他输出层加上 <code>dropout</code> 层</li>
<li><code>bidirectional</code> 默认参数是 False</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pytorch/" itemprop="url">MLDL/pytorch</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h1><p><em>参考：《深度学习入门之PyTorch》</em></p>
<h2 id="0-基本数据类型"><a href="#0-基本数据类型" class="headerlink" title="0. 基本数据类型"></a>0. 基本数据类型</h2><h3 id="0-1-数据类型"><a href="#0-1-数据类型" class="headerlink" title="0.1 数据类型"></a>0.1 数据类型</h3><p><code>torch.Tensor</code> 默认是 <code>torch.FloatTensor</code></p>
<table>
<thead>
<tr>
<th>Data tyoe</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody><tr>
<td>32-bit floating point</td>
<td>torch.FloatTensor</td>
<td>torch.cuda.FloatTensor</td>
</tr>
<tr>
<td>64-bit floating point</td>
<td>torch.DoubleTensor</td>
<td>torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>N/A</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td>torch.ByteTensor</td>
<td>torch.cuda.ByteTensor</td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td>torch.IntTensor</td>
<td>torch.cuda.IntTensor</td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td>torch.LongTensor</td>
<td>torch.cuda.LongTensor</td>
</tr>
</tbody></table>
<h3 id="0-2-对-torch-Tensor-的操作方法"><a href="#0-2-对-torch-Tensor-的操作方法" class="headerlink" title="0.2 对 torch.Tensor 的操作方法"></a>0.2 对 <code>torch.Tensor</code> 的操作方法</h3><ul>
<li><code>tensor</code>可以由python中的 <code>list</code> 或序列创建</li>
<li>也可以用python中的切片与索引来修改 <code>tensor</code> 中的内容</li>
<li>会改变 <code>tensor</code> 的函数操作会带有下划线表示，例如：<code>abs_()</code>等</li>
</ul>
<p><code>torch.max()</code> 函数用于选出 tensor 中的最大值，用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">torch.max(a)    <span class="comment"># 返回 a 中最大的元素</span></span><br><span class="line">torch.max(a)[<span class="number">1</span>] <span class="comment"># 返回 a 中最大的元素的索引</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">0</span>) <span class="comment"># 返回 a 中每一列最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">0</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一列最大的元素的索引 index</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">1</span>) <span class="comment"># 返回 a 中每一行最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一行最大的元素的索引 index</span></span><br></pre></td></tr></table></figure>

<h3 id="0-3-变换数据维度"><a href="#0-3-变换数据维度" class="headerlink" title="0.3 变换数据维度"></a>0.3 变换数据维度</h3><p>这里涉及到的总共有 3 种方法：</p>
<p><em>参考：<a href="https://zhuanlan.zhihu.com/p/76583143" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76583143</a></em></p>
<ul>
<li><code>Tensor.permute(a, b, c...)</code> 可以直接对高纬度矩阵进行转置操作</li>
<li><code>torch.transpose()</code> 与 <code>permute()</code>作用相同，但只能操作两个维度</li>
<li><code>Tensor.view()</code>; <code>view()</code> 仅能作用在连续的内存中，如果在调用了 <code>transpose()</code> 或 <code>permute()</code> 则可导致内存不连续，需要使用 <code>contiguous()</code>返回一个连续的内存拷贝；</li>
<li><code>torch.reshape()</code> <em>version &gt;=0.4</em>, <code>== tensor.contiguous().view()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a=np.array([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line">unpermuted=torch.tensor(a)</span><br><span class="line">print(unpermuted.size())              <span class="comment">#  ——&gt;  torch.Size([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">permuted=unpermuted.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">print(permuted.size())                <span class="comment">#  ——&gt;  torch.Size([3, 1, 2])</span></span><br><span class="line">print(permuted.is_contiguous())       <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">view_test = unpermuted.view(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">print(view_test.size())               <span class="comment">#  ——&gt;  torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>, <span class="number">2</span>)     <span class="comment">#  --&gt;  使用 -1 进行自动推导，包含两个维度</span></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>)        <span class="comment">#  --&gt;  只包含一个维度</span></span><br></pre></td></tr></table></figure>

<h3 id="0-4-数据拼接"><a href="#0-4-数据拼接" class="headerlink" title="0.4 数据拼接"></a>0.4 数据拼接</h3><ul>
<li><code>torch.cat(tensors, dim=0, out=None) → Tensor</code>，在指定维度上进行数据的拼接，如当<code>dim=0</code>时，在第0维度进行扩展</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>,</span><br><span class="line">         <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="0-5-数据压栈"><a href="#0-5-数据压栈" class="headerlink" title="0.5 数据压栈"></a>0.5 数据压栈</h3><ul>
<li><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></li>
</ul>
<p><code>torch.stack()</code> 要求不同的 <code>tensor</code> 之间是相同的维度，它的作用与转置的作用类似，不同的是 <code>torch.stack()</code> 作用于不同的 <code>tensor</code>，通过指定 <code>dim</code> 去除相应维度的数据进行组合成新的维度，也就是 <code>stack</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.IntTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b = torch.IntTensor([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">d0 = torch.stack([a, b], dim=<span class="number">0</span>)</span><br><span class="line">d1 = torch.stack([a, b], dim=<span class="number">1</span>)</span><br><span class="line">d2 = torch.stack([a, b], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="0-6-数据压缩"><a href="#0-6-数据压缩" class="headerlink" title="0.6 数据压缩"></a>0.6 数据压缩</h3><ul>
<li><code>tensor.squeeze()</code></li>
<li><code>tensor.unsqueeze()</code></li>
</ul>
<p>压缩的维度为元素个数为 1 的维度，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">a.size()                <span class="comment"># [2, 1]</span></span><br><span class="line">a.squeeze().size()      <span class="comment"># [2]</span></span><br></pre></td></tr></table></figure>

<h2 id="1-Variable-变量"><a href="#1-Variable-变量" class="headerlink" title="1. Variable(变量)"></a>1. Variable(变量)</h2><blockquote>
<p>Variable 是神经网络计算图里的概念，提供了自动求导的功能。Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。</p>
</blockquote>
<p>位置： <code>torch.autograd.Variable</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">1</span>]))</span><br><span class="line">w = Variable(torch.Tensor([<span class="number">2</span>]))</span><br><span class="line">b = Variable(torch.Tensor([<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">y = w * x + b</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">pritn(x.grad)</span><br></pre></td></tr></table></figure>

<h3 id="1-1-Parameters"><a href="#1-1-Parameters" class="headerlink" title="1.1 Parameters"></a>1.1 Parameters</h3><p><code>class torch.nn.Parameter()</code></p>
<p><code>Variable</code> 的一种，常被用于模块参数 <code>module parameter</code></p>
<p><code>Variable</code> 与 <code>Parameter</code>  的不同：</p>
<ul>
<li><code>Parameters</code> 是 <code>Variable</code> 的子类。 当把 <code>Parameters</code> 赋值给 <code>Modules</code>的时候，会被自动加到 <code>Module</code> 的参数列表中（会出现在 <code>parameters()</code> 迭代器中）。将 <code>Variable</code> 赋值给 <code>Module</code> 属性则不会有这样的影响。</li>
<li><code>Parameter</code> 不能被 <code>volatile</code>，且默认 <code>requires_grad=True</code>，<code>Variable</code>默认<code>requires_grad=False</code></li>
</ul>
<h2 id="2-Dataset-数据集"><a href="#2-Dataset-数据集" class="headerlink" title="2. Dataset(数据集)"></a>2. Dataset(数据集)</h2><h2 id="3-激励函数"><a href="#3-激励函数" class="headerlink" title="3. 激励函数"></a>3. 激励函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</span><br><span class="line">y_relu = torch.relu(x)</span><br><span class="line">print(y_relu)</span><br><span class="line">y_sigmoid = torch.sigmoid(x)</span><br><span class="line">print(y_sigmoid)</span><br><span class="line">y_tanh = torch.tanh(x)</span><br><span class="line">print(y_tanh)</span><br><span class="line">y_softplus = F.softplus(x)</span><br><span class="line">print(y_softplus)</span><br></pre></td></tr></table></figure>

<h2 id="4-Commonly-used-function"><a href="#4-Commonly-used-function" class="headerlink" title="4. Commonly used function"></a>4. Commonly used function</h2><ul>
<li><code>torch.view()</code> 类似于 <code>numpy.reshape()</code> 但不同的是，<code>view()</code> 函数并不进行变量内存的复制，而只是在原来的内存区域进行操作。</li>
<li><code>torch.Tensor()</code></li>
<li><code>torch.LongTensor()</code></li>
<li><code>torch.optim.SGD()</code></li>
</ul>
<h2 id="5-Containers（容器）-torch-nn-Module"><a href="#5-Containers（容器）-torch-nn-Module" class="headerlink" title="5. Containers（容器） torch.nn Module"></a>5. Containers（容器） <code>torch.nn</code> Module</h2><h3 id="5-1-nn-Linear-in-features-out-features-bias-True"><a href="#5-1-nn-Linear-in-features-out-features-bias-True" class="headerlink" title="5.1 nn.Linear(in_features, out_features, bias=True)"></a>5.1 <code>nn.Linear(in_features, out_features, bias=True)</code></h3><p>$$y=xA^{T}+b$$</p>
<h4 id="5-1-1-Parameters"><a href="#5-1-1-Parameters" class="headerlink" title="5.1.1 Parameters"></a>5.1.1 Parameters</h4><ul>
<li><code>in_features</code> 每个输入样本的大小</li>
<li><code>out_features</code> 每个输出样本的大小</li>
</ul>
<h4 id="5-1-2-Shape"><a href="#5-1-2-Shape" class="headerlink" title="5.1.2 Shape"></a>5.1.2 Shape</h4><ul>
<li><p><code>input:</code><br>$$(N, *, H_{in}), 其中H_{in} = in_features$$</p>
</li>
<li><p><code>output:</code><br>$$(N, *, H_{out}), 其中H_{out} = out_features$$</p>
</li>
</ul>
<h4 id="5-1-3-Example"><a href="#5-1-3-Example" class="headerlink" title="5.1.3 Example"></a>5.1.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure>

<h3 id="5-2-nn-Dropout-p-0-5-inplace-False"><a href="#5-2-nn-Dropout-p-0-5-inplace-False" class="headerlink" title="5.2 nn.Dropout(p=0.5, inplace=False)"></a>5.2 <code>nn.Dropout(p=0.5, inplace=False)</code></h3><ul>
<li><code>p</code> 将元素置0的概率</li>
<li><code>inplace</code> 若设置为 True，会在原地进行操作</li>
</ul>
<h3 id="5-3-nn-ReLU-inplace-False"><a href="#5-3-nn-ReLU-inplace-False" class="headerlink" title="5.3 nn.ReLU(inplace=False)"></a>5.3 <code>nn.ReLU(inplace=False)</code></h3><p>$$<br>y=\begin{cases}<br>0, \quad x\leq0 \\<br>x, \quad x&gt;0<br>\end{cases}<br>$$</p>
<h3 id="5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False"><a href="#5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False" class="headerlink" title="5.4 nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)"></a>5.4 <code>nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)</code></h3><p>保存了固定字典和大小的简单查找表，该模块保存，这里只是初始化的一些向量，后续还需要进行学习和修改</p>
<ul>
<li><code>num_embeddings</code> 嵌入字典的大小</li>
<li><code>embeddings_dim</code> 每个嵌入向量的大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;<span class="string">'hello'</span>: <span class="number">0</span> ,<span class="string">'world'</span>: <span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">hello_idx = torch.LongTensor([word_to_idx[<span class="string">'hello'</span>]])</span><br><span class="line">hello_variable = Variable(hello_idx)</span><br><span class="line">hello_embed = embeds(hello_variable)</span><br><span class="line">print(hello_embed)</span><br><span class="line"></span><br><span class="line">embeds.weight.data = torch.ones(<span class="number">2</span>, <span class="number">5</span>)   <span class="comment"># embeddings 的 weight 可以设置 </span></span><br><span class="line">print(embeds.weight)</span><br></pre></td></tr></table></figure>

<h3 id="5-5-nn-Sequential-args"><a href="#5-5-nn-Sequential-args" class="headerlink" title="5.5 nn.Sequential(*args)"></a>5.5 <code>nn.Sequential(*args)</code></h3><p>一个时序容器。 <code>Modules</code> 会以他们传入的顺序被添加到容器中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu2'</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>

<h3 id="5-6-nn-CrossEntropyLoss"><a href="#5-6-nn-CrossEntropyLoss" class="headerlink" title="5.6 nn.CrossEntropyLoss"></a>5.6 <code>nn.CrossEntropyLoss</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=<span class="number">-100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">'mean'</span>)</span><br></pre></td></tr></table></figure>

<p>该函数相当于组合了 <code>nn.LogSoftmax()</code> 和 <code>nn.NLLLoss()</code></p>
<p>$$loss(x,class) = -log{\frac{exp(x[class])}{\sum_{j}exp(x[j])}}$$</p>
<p>等于（当 <code>weight</code> 参数非零时）</p>
<p>$$loss(x,class) = weight[class]\left(-x[class] + log(\sum_{j}exp\left(x[j]\right)\right)$$</p>
<p>通过以上公式求交叉熵，应用于多分类问题中的<code>loss function</code></p>
<h4 id="5-6-1-Parameters"><a href="#5-6-1-Parameters" class="headerlink" title="5.6.1 Parameters"></a>5.6.1 Parameters</h4><ul>
<li><code>weight</code>，结合上式，赋值给每个class 的 weight</li>
<li><code>reduction</code> 默认为 <code>mean</code>，可以为 <code>none | mean | sum</code>，是对计算后的 output 值的整合</li>
</ul>
<h4 id="5-6-2-Shape"><a href="#5-6-2-Shape" class="headerlink" title="5.6.2 Shape"></a>5.6.2 Shape</h4><ul>
<li>输入1：<code>Input:(N,C)</code> N 代表batch个数，C 代表 class 的个数；</li>
<li>输入2：<code>Target:(N)</code> 代表对应的类别的序号</li>
</ul>
<h4 id="5-6-3-Example"><a href="#5-6-3-Example" class="headerlink" title="5.6.3 Example"></a>5.6.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 输入 batch:3,class numbers: 5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>) <span class="comment"># 输入 target:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(input, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure>

<h3 id="5-7-nn-RNN"><a href="#5-7-nn-RNN" class="headerlink" title="5.7 nn.RNN()"></a>5.7 <code>nn.RNN()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.RNN(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>可以构造多层的RNN，每一层的计算函数为：</p>
<p>$$h_{t} = tanh\left( W_{ih}x_{t} + b_{ih} + W_{hh}h_{(t-1)} + b_{hh}\right)$$</p>
<h4 id="5-7-1-Parameters"><a href="#5-7-1-Parameters" class="headerlink" title="5.7.1 Parameters"></a>5.7.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>nonlinearity:</code> 非线性函数，可以为 <code>tanh</code> 或 <code>relu</code>，默认为 <code>tanh</code></li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional RNN</li>
</ul>
<h4 id="5-7-2-Shape"><a href="#5-7-2-Shape" class="headerlink" title="5.7.2 Shape"></a>5.7.2 Shape</h4><ul>
<li><p><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</p>
</li>
<li><p><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</p>
</li>
<li><p><code>output</code> shape(seq_len, batch, num_directions * hidden_size)，这里的 output 是最后一层 layer 的 output</p>
</li>
</ul>
<h4 id="5-7-3-Example"><a href="#5-7-3-Example" class="headerlink" title="5.7.3 Example"></a>5.7.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.RNN(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)     <span class="comment"># input_size: 10; hidden_size: 20; num_layers: 2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># seq_len: 5; batch: 3; input_size: 10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>) <span class="comment"># num_layers * num_directions: 2; batch: 3; hidden_size: 20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<p><strong>Notation:</strong></p>
<ol>
<li><code>RNN</code> 的 output：是最后一层（the last hidden_layer）的每个 <code>batch</code> 每个 <code>time_step/seq_len</code> 的output</li>
<li><code>RNN</code> 的 h_n 是每一个隐藏层（hidden_layer）的每个 <code>batch</code> 的 output</li>
</ol>
<h3 id="5-8-nn-LSTM"><a href="#5-8-nn-LSTM" class="headerlink" title="5.8 nn.LSTM()"></a>5.8 <code>nn.LSTM()</code></h3><p>$$<br>\begin{aligned}<br>i_{t} =&amp; \ \sigma\left(W_{ii}x_{t} + b_{ii} + W_{hi}h_{(t-1)} + b_{hi}\right) \<br>f_{t} =&amp; \ \sigma\left(W_{if}x_{t} + b_{if} + W_{hf}h_{(t-1)} + b_{hf}\right) \<br>g_{t} =&amp; \ tanh(W_{ig}x_{t} + b_{ig} + W_{hg}h_{(t-1)} + b_{hg}) \<br>o_{t} =&amp; \ \sigma(W_{io}x_{t} + b_{io} + W_{ho}h_{(t-1)} + b_{ho}) \<br>c_{t} =&amp; \ f_{t} * c_{(t-1)} + i_{t} * g_{t} \<br>h_{t} =&amp; \ o_{t} * tanh(c_{t})<br>\end{aligned}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LSTM(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<h4 id="5-8-1-Parameters"><a href="#5-8-1-Parameters" class="headerlink" title="5.8.1 Parameters"></a>5.8.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-8-2-Shape"><a href="#5-8-2-Shape" class="headerlink" title="5.8.2 Shape"></a>5.8.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
<li><code>c_0:</code> shape(num_layers*num_directions, batch, hidden_size), 初始化的 cell state 每个 batch</li>
</ul>
<h4 id="5-8-3-Example"><a href="#5-8-3-Example" class="headerlink" title="5.8.3 Example"></a>5.8.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.LSTM(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, (hn, cn) = rnn(input, (h0, c0))</span><br></pre></td></tr></table></figure>

<h3 id="5-9-nn-GRU"><a href="#5-9-nn-GRU" class="headerlink" title="5.9 nn.GRU()"></a>5.9 <code>nn.GRU()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.GRU(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>$$<br>\begin{aligned}<br>r_{t} =&amp; \ \sigma(W_{ir}x_{t} + b_{ir} + W_{hr}h_{(t-1)} + b_{hr}) \<br>z_{t} =&amp; \ \sigma(W_{iz}x_{t} + b_{iz} + W_{hz}h_{(t-1)} + b_{hz}) \<br>n_{t} =&amp; \ tanh(W_{in}x_{t} + b_{in} + r_{t}*(W_{hn}h_{(t-1)} + b_{hn})) \<br>h_{t} =&amp; \ (1-z_{t}) * n_{t} + z_{t} * h_{(t-1)}<br>\end{aligned}<br>$$</p>
<h4 id="5-9-1-Parameters"><a href="#5-9-1-Parameters" class="headerlink" title="5.9.1 Parameters"></a>5.9.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-9-2-Shape"><a href="#5-9-2-Shape" class="headerlink" title="5.9.2 Shape"></a>5.9.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
</ul>
<h4 id="5-9-3-Example"><a href="#5-9-3-Example" class="headerlink" title="5.9.3 Example"></a>5.9.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<h2 id="6-torch-optim"><a href="#6-torch-optim" class="headerlink" title="6. torch.optim"></a>6. <code>torch.optim</code></h2><p><code>torch.optim</code> 是一个实现了各种优化算法的库。</p>
<h3 id="6-1-如何使用-optimizer"><a href="#6-1-如何使用-optimizer" class="headerlink" title="6.1 如何使用 optimizer"></a>6.1 如何使用 <code>optimizer</code></h3><p>为了使用 <code>torch.optim</code>，你需要构建一个 optimizer 对象，这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。</p>
<ul>
<li>构建</li>
</ul>
<p>为了构建一个 <code>optimizer</code>，需要传入一个包含优化参数（必须都是 <code>Variable</code> 对象）的 iterable，并设置 optimizer 的参数选项，如：学习率、权重衰减等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, )</span><br></pre></td></tr></table></figure>

<ul>
<li>进行单次优化</li>
</ul>
<p>采用的方法：</p>
<p><code>optimizer.step()</code> 这个方法会更新所有的参数，所有的 optimizer 都实现了这个方法，一旦梯度被 <code>backward()</code> 之类的函数计算好后，就可以调用这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单次优化的常用做法</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<h3 id="6-2-常用函数"><a href="#6-2-常用函数" class="headerlink" title="6.2 常用函数"></a>6.2 常用函数</h3><ul>
<li><code>step(closure)</code> 进行单次优化</li>
<li><code>zero_grad()</code> 清空所有被优化过的 Variable 的梯度</li>
</ul>
<h2 id="7-torch-utils"><a href="#7-torch-utils" class="headerlink" title="7. torch.utils"></a>7. <code>torch.utils</code></h2><h3 id="7-1-torch-utils-data-Dataloader"><a href="#7-1-torch-utils-data-Dataloader" class="headerlink" title="7.1 torch.utils.data.Dataloader"></a>7.1 <code>torch.utils.data.Dataloader</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>主要的几个参数：</p>
<ul>
<li>dataset: 数据集来源，可以是 <code>map-style</code> or <code>iterable-style</code> dataset.</li>
<li>batch_size: 每个 <code>batch</code> 有多少个 samples</li>
<li>shuffle: 是否在每个 <code>epoch</code> 进行 reshuffle</li>
</ul>
<p>python 迭代器构造在数据集上。</p>
<h2 id="8-模型的保存与加载"><a href="#8-模型的保存与加载" class="headerlink" title="8. 模型的保存与加载"></a>8. 模型的保存与加载</h2><p><em>参考：<a href="https://zhuanlan.zhihu.com/p/38056115" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38056115</a></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存整个网络</span></span><br><span class="line">torch.save(net, PATH)</span><br><span class="line"><span class="comment"># 保存网络中的参数</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型的加载，对应于以上两种保存的方式</span></span><br><span class="line">model_dict = torch.load(PATH)</span><br><span class="line">model_dict = model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure>

<h2 id="9-参考"><a href="#9-参考" class="headerlink" title="9. 参考"></a>9. 参考</h2><ul>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/" target="_blank" rel="noopener">PyTorch-官方教程</a></li>
<li><a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">pytorch-文档</a>    </li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pytorch(2)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pytorch(2)/" itemprop="url">MLDL/pytorch(2)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pytorch-二"><a href="#pytorch-二" class="headerlink" title="pytorch (二)"></a>pytorch (二)</h1><h2 id="1-Variable-Tensor-numpy-之间的转换"><a href="#1-Variable-Tensor-numpy-之间的转换" class="headerlink" title="1. Variable Tensor numpy 之间的转换"></a>1. <code>Variable</code> <code>Tensor</code> <code>numpy</code> 之间的转换</h2><p><strong>Variable 已经被 depreciate 了！！！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy --&gt; tensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor_variable = torch.from_numpy(numpy_variable)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor --&gt; numpy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>numpy_variable = tensor_variable.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy -- &gt; Tensor --&gt; Variavle</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v = Variable(torch.from_numpy(numpy_variable))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable --&gt; numpy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>numpy_variable = v.data.numpy()</span><br></pre></td></tr></table></figure>

<h2 id="2-tensor-new"><a href="#2-tensor-new" class="headerlink" title="2. tensor.new()"></a>2. <code>tensor.new()</code></h2><p><em><a href="https://pytorch.org/docs/0.3.1/tensors.html?highlight=new#torch.Tensor.new" target="_blank" rel="noopener">source</a></em></p>
<p><code>new(*args, **kwargs)</code></p>
<p>Constructs a new tensor of the same data type as self tensor.</p>
<blockquote>
<p>For CUDA tensors, this method will create new tensor on the same device as this tensor.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = (input.data.new([word2ix[w]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br></pre></td></tr></table></figure>

<h2 id="3-cuda-上-tensor-的定义"><a href="#3-cuda-上-tensor-的定义" class="headerlink" title="3. cuda 上 tensor 的定义"></a>3. <code>cuda</code> 上 <code>tensor</code> 的定义</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.one(<span class="number">1000</span>, <span class="number">1000</span>, <span class="number">3</span>).cuda()     <span class="comment"># 在 cpu 上定义，将数据转移到 cuda 上</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros().cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在某一GPU设备上定义</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cuda1 = torch.deivce(<span class="string">'cuda:1'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn((<span class="number">1000</span>, <span class="number">1000</span>, <span class="number">1000</span>), device=cuda1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接在 GPU上定义数据，减少cpu的损耗</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.FloatTensor(batch_size, self.hidden_dim, self.height, self.width).fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<h2 id="4-torch-上的一些数学运算"><a href="#4-torch-上的一些数学运算" class="headerlink" title="4. torch 上的一些数学运算"></a>4. <code>torch</code> 上的一些数学运算</h2><h3 id="4-1-torch-bmm-torch-mm-torch-matmul-torch-mul"><a href="#4-1-torch-bmm-torch-mm-torch-matmul-torch-mul" class="headerlink" title="4.1 torch.bmm(),torch.mm(),torch.matmul(),torch.mul()"></a>4.1 <code>torch.bmm(),torch.mm(),torch.matmul(),torch.mul()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵之间的相乘，叉乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mm(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.matmul(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 矩阵的 batch 叉乘运算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = torch.bmm(batch1, batch2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.size()</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 矩阵各元素之间的相乘，是对应位之间的相乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(mat1, mat2)</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pyplot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pyplot/" itemprop="url">MLDL/pyplot</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pyplot"><a href="#pyplot" class="headerlink" title="pyplot"></a>pyplot</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<p>使用该库绘制图形</p>
<h2 id="2-矩阵-数组的可视化"><a href="#2-矩阵-数组的可视化" class="headerlink" title="2. 矩阵/数组的可视化"></a>2. 矩阵/数组的可视化</h2><p><code>matshow()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samplemat</span><span class="params">(dims)</span>:</span></span><br><span class="line">    <span class="string">"""Make a matrix with all zeros and increasing elements on the diagonal"""</span></span><br><span class="line">    aa = np.zeros(dims)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(min(dims)):</span><br><span class="line">        aa[i, i] = i</span><br><span class="line">    <span class="keyword">return</span> aa</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Display matrix</span></span><br><span class="line">plt.matshow(samplemat((<span class="number">15</span>, <span class="number">15</span>)))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/numpy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/numpy/" itemprop="url">MLDL/numpy</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Numpy-学习"><a href="#Numpy-学习" class="headerlink" title="Numpy 学习"></a>Numpy 学习</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="1-函数"><a href="#1-函数" class="headerlink" title="1. 函数"></a>1. 函数</h2><h3 id="1-1-生成序列"><a href="#1-1-生成序列" class="headerlink" title="1.1 生成序列"></a>1.1 生成序列</h3><ul>
<li><code>range()</code> 可生成一定 step 的序列，但只能整数</li>
<li><code>arange()</code> 可生成一定 step 的序列，可以是浮点数</li>
</ul>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><h2 id="2-np-eye-与-np-identity"><a href="#2-np-eye-与-np-identity" class="headerlink" title="2. np.eye() 与 np.identity()"></a>2. <code>np.eye()</code> 与 <code>np.identity()</code></h2><p><code>numpy.eye(N, M=None, k=0, dtype=&lt;class &#39;float&#39;&gt;, order=&#39;C&#39;)[source]</code></p>
<p>可以生成2-D的对角矩阵（不一定对角），零可以被设置放置在任意地方；</p>
<h3 id="2-1-Example"><a href="#2-1-Example" class="headerlink" title="2.1 Example"></a>2.1 Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">2</span>, dtype=int)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>, k=<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 常用的可以对数组进行 one-hot 编码</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>)[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]]</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<p><code>np.identity(n, dtype=None)</code></p>
<p>仅能生成对角方阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.identity(<span class="number">3</span>)</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/Jupyter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/Jupyter/" itemprop="url">MLDL/Jupyter</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Jupyter-的使用"><a href="#Jupyter-的使用" class="headerlink" title="Jupyter 的使用"></a>Jupyter 的使用</h1><p>Jupyter 通篇内容有两种格式：Markdown 与 code，Markdown 模式下可以进行 Markdown 的编辑，code 模式下则可以进行代码的运行，这样有机地将代码和注释结合起来。</p>
<h2 id="Markdown-模式"><a href="#Markdown-模式" class="headerlink" title="Markdown 模式"></a>Markdown 模式</h2><p><code>shift</code>+<code>enter</code> 快捷键是个很重要的快捷键，在 Markdown 模式下使用则可以进行预览和编辑的切换</p>
<h2 id="Code-模式"><a href="#Code-模式" class="headerlink" title="Code 模式"></a>Code 模式</h2><p><code>shift</code>+<code>enter</code> 快捷键在 <code>code/md</code> 模式下可以进行代码/字体的创建和运行</p>
<h2 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h2><ul>
<li><code>H</code>：查看所有快捷键。</li>
<li><code>S</code>：保存当前 Notebook 内容。</li>
<li><code>P</code>：调出 Notebook 命令栏。</li>
</ul>
<h3 id="对单元格（Cell）的操作"><a href="#对单元格（Cell）的操作" class="headerlink" title="对单元格（Cell）的操作"></a>对单元格（Cell）的操作</h3><ul>
<li><code>A/a</code>: 在当前单元格上方新建单元格 cell</li>
<li><code>B/b</code>：在当前单元格下方新建空白单元格。</li>
<li><code>M</code>：将单元格格式转换为 Markdown。</li>
<li><code>Y</code>：将单元格格式转换为 Code。</li>
<li><code>连续按 D+D/ d+d</code>：删除当前单元格。（慎用，推荐使用 X 剪切单元格代替，因为其可以起到删除效果，且删错了还可以粘贴回来）</li>
<li><code>Shift + Enter</code>：运行当前单元格内容。（当 Markdown 单元格处于编辑状态时，运行即可复原)</li>
<li><code>ESC</code> 在编辑模式（绿色）与非编辑模式（蓝色）下切换</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/Ipdb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/Ipdb/" itemprop="url">MLDL/Ipdb</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="IPDB-Ipython-Debugger"><a href="#IPDB-Ipython-Debugger" class="headerlink" title="IPDB (Ipython Debugger)"></a>IPDB (Ipython Debugger)</h1><p>IPDB和GDB类似，是一款集成了<code>Ipython</code>的Python代码命令行调试工具。</p>
<h2 id="1-集成到源代码中使用"><a href="#1-集成到源代码中使用" class="headerlink" title="1. 集成到源代码中使用"></a>1. 集成到源代码中使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"></span><br><span class="line">x = <span class="number">10</span></span><br><span class="line">ipdb.set_trace() <span class="comment"># 执行到此，程序停止，展开Ipython环境</span></span><br><span class="line">y = <span class="number">20</span></span><br></pre></td></tr></table></figure>

<h2 id="2-交互式使用"><a href="#2-交互式使用" class="headerlink" title="2. 交互式使用"></a>2. 交互式使用</h2><p><code>python -m ipdb code.py</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/CUDA/" itemprop="url">MLDL/CUDA</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>暑假实习的是关于一个 GPU 开发库的Python版本项目，该项目 C++ 版本与 Python 版本进行同步开发，Python版本使用 <code>swig</code> 进行 C++ interface 的 wrap，这些 GPU 编程库往往都有很多相似的地方，当时借鉴了很多 CUDA 的相关资料，这里进行记录</p>
<h3 id="1-2-高效的-GPU-任务具备条件"><a href="#1-2-高效的-GPU-任务具备条件" class="headerlink" title="1.2 高效的 GPU 任务具备条件"></a>1.2 高效的 GPU 任务具备条件</h3><ol>
<li>具有成千上万的独立工作<ul>
<li>尽量利用大量的 ALU 单元</li>
<li>大量的片元切换隐延迟</li>
</ul>
</li>
<li>可以共享指令流<ul>
<li>适用于 SIMD 处理</li>
</ul>
</li>
<li>最好是计算密集型任务<ul>
<li>通信和计算开销比例适合</li>
<li>不要受制于访存带宽</li>
</ul>
</li>
</ol>
<p><strong>CUDA这一部分应该掌握GPU软硬件之间的对应关系</strong></p>
<h2 id="2-GPU的体系结构"><a href="#2-GPU的体系结构" class="headerlink" title="2. GPU的体系结构"></a>2. GPU的体系结构</h2><h3 id="2-1-硬件"><a href="#2-1-硬件" class="headerlink" title="2.1 硬件"></a>2.1 硬件</h3><ul>
<li>SP（Streaming Processor），亦称 CUDA core，指令和任务的实际执行者</li>
<li>SM（Streaming Multiprocessor），GPu大核，拥有除SP外的其它资源如： wrap scheduler、register、shared memory等。</li>
</ul>
<p>展示GPU的一个SM的示意图（绿色部分为 SP）：</p>
<p><img src="./IMG/cuda_sm.png" alt="cuda_sm"></p>
<p>GPU 中的内存按照访存速度划分：</p>
<ul>
<li>Register</li>
<li>shared memory (block)</li>
<li>local memory</li>
<li>global memory (device)</li>
<li>constant memory (device)</li>
<li>texture memory</li>
<li>instruction memory (invisible)</li>
</ul>
<h3 id="2-2-软件"><a href="#2-2-软件" class="headerlink" title="2.2 软件"></a>2.2 软件</h3><p>软件的三级结构： </p>
<ul>
<li>thread</li>
<li>block</li>
<li>grid</li>
</ul>
<p><img src="./IMG/cuda_sw.png" alt="cuda_sw"></p>
<h3 id="2-3-软硬件之间的对应关系"><a href="#2-3-软硬件之间的对应关系" class="headerlink" title="2.3 软硬件之间的对应关系"></a>2.3 软硬件之间的对应关系</h3><p><img src="./IMG/cuda_sw_hw.png" alt="cuda_sw_hw"></p>
<h2 id="3-CUDA的编程模型"><a href="#3-CUDA的编程模型" class="headerlink" title="3. CUDA的编程模型"></a>3. CUDA的编程模型</h2><h3 id="3-1-CUDA-的算法框架"><a href="#3-1-CUDA-的算法框架" class="headerlink" title="3.1 CUDA 的算法框架"></a>3.1 CUDA 的算法框架</h3><p>1) 在 GPU 上分配显存，拷贝数据 host to device<br>2) kernel invocation code<br>3) 运算结束后，输出数据拷贝 device to host</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/junparadox/article/details/50540602" target="_blank" rel="noopener">CUDA编程</a></li>
<li><a href="https://www.bilibili.com/video/av10436982?from=search&seid=450076380525809673" target="_blank" rel="noopener">《CUDA编程教程》 B站视频</a></li>
<li>《CUDA并行程序设计 GPU编程指南》</li>
<li><a href="https://www.easyhpc.net/lab/detail/22/" target="_blank" rel="noopener">超算课堂-CUDA编程实训</a> </li>
<li><a href="https://devblogs.nvidia.com/easy-introduction-cuda-c-and-c/" target="_blank" rel="noopener">Nvidia-introductoion-of-cuda-c-and-c</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/conda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/conda/" itemprop="url">MLDL/conda</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:38:21+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Conda"><a href="#Conda" class="headerlink" title="Conda"></a>Conda</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看conda下的所有基本配置</span></span><br><span class="line">conda config --show</span><br><span class="line"><span class="comment"># 删除conda的channel</span></span><br><span class="line">conda config --remove channels <span class="string">'default'</span></span><br><span class="line"><span class="comment"># 添加conda的channel</span></span><br><span class="line">conda config --add channels https://xxx</span><br><span class="line"><span class="comment"># 配置清华PyPI镜像（如无法运行，将pip版本升级到&gt;=10.0.0）</span></span><br><span class="line">pip config set <span class="keyword">global</span>.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"><span class="comment"># 创建环境</span></span><br><span class="line">conda create --name envname python=<span class="number">3.6</span></span><br><span class="line"><span class="comment"># 激活环境</span></span><br><span class="line">conda activate envname</span><br><span class="line"><span class="comment"># 获取当前环境中已经安装的包</span></span><br><span class="line">conda list</span><br><span class="line"><span class="comment"># 获取某个环境中安装的包</span></span><br><span class="line">conda list -n envname</span><br><span class="line"><span class="comment"># 退出环境</span></span><br><span class="line">conda deactivate</span><br><span class="line"><span class="comment"># 删除环境</span></span><br><span class="line">conda remove -n envname --all</span><br><span class="line"><span class="comment"># 导出环境</span></span><br><span class="line">conda env export &gt; environment.yml</span><br><span class="line"><span class="comment"># 根据 .yml 文件创建环境</span></span><br><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure>

<h2 id="pip-源"><a href="#pip-源" class="headerlink" title="pip 源"></a>pip 源</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt <span class="comment"># pip 清华源</span></span><br><span class="line">pip install -i http://pypi.douban.com/simple/ -r requirements.txt           <span class="comment"># pip 豆瓣源</span></span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/57287956" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57287956</a></li>
<li></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/15/">&lt;i class=&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.png"
                alt="wanncy" />
            
              <p class="site-author-name" itemprop="name">wanncy</p>
              <p class="site-description motion-element" itemprop="description">千淘万漉虽辛苦，吹尽黄沙始到金</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">159</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wanncy</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
