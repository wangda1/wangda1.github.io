<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="千淘万漉虽辛苦，吹尽黄沙始到金">
<meta property="og:type" content="website">
<meta property="og:title" content="笔记记录">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;page&#x2F;7&#x2F;index.html">
<meta property="og:site_name" content="笔记记录">
<meta property="og:description" content="千淘万漉虽辛苦，吹尽黄沙始到金">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/7/"/>





  <title>笔记记录</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">笔记记录</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/" itemprop="url">中文事件抽取综述</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:47:57+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Course/" itemprop="url" rel="index">
                    <span itemprop="name">Course</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="中文事件抽取技术研究"><a href="#中文事件抽取技术研究" class="headerlink" title="中文事件抽取技术研究"></a>中文事件抽取技术研究</h2><p><em>C:/Users/wangc/Desktop/newsGrid/paper/事件抽取</em></p>
<h3 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h3><h4 id="1-1-事件抽取的三大关键技术："><a href="#1-1-事件抽取的三大关键技术：" class="headerlink" title="1.1 事件抽取的三大关键技术："></a>1.1 事件抽取的三大关键技术：</h4><ul>
<li>EMD（Entity Mention Detection），实体识别</li>
<li>ED（Event Detection），事件的识别</li>
<li>ARP（Argument Role Prediction），论元角色的识别</li>
</ul>
<h4 id="1-2-事件抽取的方法："><a href="#1-2-事件抽取的方法：" class="headerlink" title="1.2 事件抽取的方法："></a>1.2 事件抽取的方法：</h4><ul>
<li>基于模式匹配的方法</li>
<li>基于机器学习的方法</li>
</ul>
<h4 id="1-3-基于机器学习的方法把事件抽取的任务看作分类问题，"><a href="#1-3-基于机器学习的方法把事件抽取的任务看作分类问题，" class="headerlink" title="1.3 基于机器学习的方法把事件抽取的任务看作分类问题，"></a>1.3 基于机器学习的方法把事件抽取的任务看作分类问题，</h4><blockquote>
<p>Hai Leong Chieu 和 Hwee Tou<br>Ng 于 2002 年首次在事件抽取中引入最大熵分类<br>器[9 ] ,用于事件元素的识别 ; David Ahn 2006 年结<br>合 MegaM 和 Timbl 两种机器学习方法分别实现了<br>事件抽取中事件类别识别和事件元素识别这两个主<br>要步骤 , 在 ACE 英文语料上均取得了不错的效<br>果[4 ] 。 但 Ahn 的方法由于将每个词作为一个实例<br>来训练机器学习模型 ,引入了大量的反例 ,导致正反<br>例严重不平衡 ;此外 ,事件类别的多元分类以及为每<br>类事件元素单独构造多元分类器在语料规模较小的<br>时候存在着一定的数据稀疏问题 。</p>
</blockquote>
<h3 id="2-Method"><a href="#2-Method" class="headerlink" title="2. Method"></a>2. Method</h3><blockquote>
<p>本文提出一种基于触发<br>词扩展和二元分类相结合的识别方法进行事件类别<br>的识别 ,多元分类模型的方法进行事件元素的识别 ,<br>较好的避免了正反例不平衡和数据稀疏问题 。</p>
</blockquote>
<p><strong>keywords: 基于触发词扩展和二元分类的事件识别，多元分类用于事件元素识别</strong></p>
<h4 id="2-1-系统架构图"><a href="#2-1-系统架构图" class="headerlink" title="2.1 系统架构图"></a>2.1 系统架构图</h4><p><img src="/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/Paper_%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B01.png" alt="系统架构图"></p>
<h4 id="2-2-事件类别的识别"><a href="#2-2-事件类别的识别" class="headerlink" title="2.2 事件类别的识别"></a>2.2 事件类别的识别</h4><p>a. 通过对句子的分词查看扩充之后的触发词表，并对每一个候选事件划定一个候选事件类别的范围；</p>
<p>b. 通过将候选事件类别识别看作一个二元分类的问题，判断是否为满足候选类别的事件；<br><img src="/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/Paper_%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B02.png" alt="特征描述"></p>
<h4 id="2-3-事件元素的识别"><a href="#2-3-事件元素的识别" class="headerlink" title="2.3 事件元素的识别"></a>2.3 事件元素的识别</h4><p>将事件元素的识别任务看成分类问题，转换为对文本中每个候选元素进行类别标签的识别，再进行挑选</p>
<p>a. 三种多元分类的策略</p>
<p><img src="/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/Paper_%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B03.png" alt="分类策略"></p>
<p>b. 特征的选取</p>
<p><img src="/2020/01/04/NLP/Course/%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B0/Paper_%E4%B8%AD%E6%96%87%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96%E7%BB%BC%E8%BF%B04.png" alt="分类特征"></p>
<h3 id="3-Performance"><a href="#3-Performance" class="headerlink" title="3. Performance"></a>3. Performance</h3><p>使用 ACE 2005 中文语料作为实验数据，使用 F值的评价方法进行评测。</p>
<h3 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4. Conclusion"></a>4. Conclusion</h3><p>这篇文章采用的是对事件识别和事件论元识别的分类方法，一种典型的 Pipe Model，在模型的改进方面并没有多大的创新。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/" itemprop="url">新闻推荐系统_武楚涵</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:46:06+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Course/" itemprop="url" rel="index">
                    <span itemprop="name">Course</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="新闻推荐"><a href="#新闻推荐" class="headerlink" title="新闻推荐"></a>新闻推荐</h1><p><em>清华大学武楚涵博士</em></p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu01.jpg" alt="wu_01"></p>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu02.jpg" alt="wu_02"></p>
<ol>
<li>Google News Personalization: Scalable Online Collaborative Filtering”, WWW’07, pp. 271-280, 2007.</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu03.jpg" alt="wu_03"></p>
<ol>
<li>Jeong Woo Son, A.-Yeong Kim, Seong-Bae Park:<br>A location-based news article recommendation with explicit localized semantic analysis. SIGIR 2013: 293-302</li>
<li>Trapit Bansal, Mrinal Kanti Das, Chiranjib Bhattacharyya:<br>Content Driven User Profiling for Comment-Worthy Recommendations of News and Blog Articles. RecSys 2015: 195-202</li>
<li>Jianxun Lian, Fuzheng Zhang, Xing Xie, Guangzhong Sun:<br>Towards Better Representation Learning for Personalized News Recommendation: a Multi-Channel Deep Fusion Approach. IJCAI 2018: 3805-3811</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu04.jpg" alt="wu_04"></p>
<ol>
<li>Shumpei Okura, Yukihiro Tagami, Shingo Ono, Akira Tajima:<br>Embedding-based News Recommendation for Millions of Users. 1933-1942</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu05.jpg" alt="wu_05"></p>
<ol>
<li>Hongwei Wang, Fuzheng Zhang, Xing Xie, Minyi Guo:<br>DKN: Deep Knowledge-Aware Network for News Recommendation. WWW 2018: 1835-1844</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu06.jpg" alt="wu_06"></p>
<ol>
<li>Qiannan Zhu, Xiaofei Zhou, Zeliang Song, Jianlong Tan, Li Guo:<br>DAN: Deep Attention Neural Network for News Recommendation. AAAI 2019: 5973-5980</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu07.jpg" alt="wu_07"></p>
<ol>
<li>Dhruv Khattar, Vaibhav Kumar, Vasudeva Varma, Manish Gupta:<br>Weave&amp;Rec: A Word Embedding based 3-D Convolutional Network for News Recommendation. CIKM 2018: 1855-1858</li>
</ol>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu08.jpg" alt="wu_08"></p>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu09.jpg" alt="wu_09"></p>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu10.jpg" alt="wu_10"></p>
<p><img src="/2020/01/04/NLP/Course/%E6%96%B0%E9%97%BB%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F_%E6%AD%A6%E6%A5%9A%E6%B6%B5/wu11.jpg" alt="wu_11"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/" itemprop="url">事件抽取_陈玉博</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T22:45:08+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/Course/" itemprop="url" rel="index">
                    <span itemprop="name">Course</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="事件抽取-讲座"><a href="#事件抽取-讲座" class="headerlink" title="事件抽取-讲座"></a>事件抽取-讲座</h1><p><em>主讲人：陈玉博，中科院自动化所，DmCNN的一作</em></p>
<p><em>参考CSDN公开课视频：<a href="https://edu.csdn.net/course/play/25560/307899" target="_blank" rel="noopener">https://edu.csdn.net/course/play/25560/307899</a></em></p>
<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_01.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_02.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_03.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_04.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_05.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_06.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_07.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_08.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_09.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_10.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_11.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_12.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_13.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_14.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_15.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_16.png" alt="chen_ee"></p>
<h2 id="1-特征表示"><a href="#1-特征表示" class="headerlink" title="1. 特征表示"></a>1. 特征表示</h2><p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_17.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_18.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_19.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_20.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_21.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_22.png" alt="chen_ee"></p>
<h2 id="2-训练数据生成和扩展"><a href="#2-训练数据生成和扩展" class="headerlink" title="2. 训练数据生成和扩展"></a>2. 训练数据生成和扩展</h2><p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_23.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_24.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_25.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_26.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_27.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_28.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_29.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_30.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_31.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_32.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_33.png" alt="chen_ee"></p>
<h2 id="3-多事件协同抽取"><a href="#3-多事件协同抽取" class="headerlink" title="3. 多事件协同抽取"></a>3. 多事件协同抽取</h2><p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_34.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_35.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_36.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_37.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_38.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_39.png" alt="chen_ee"></p>
<h2 id="4-篇章级事件抽取"><a href="#4-篇章级事件抽取" class="headerlink" title="4. 篇章级事件抽取"></a>4. 篇章级事件抽取</h2><p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_40.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_41.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_42.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_43.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_44.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_45.png" alt="chen_ee"></p>
<p><img src="/2020/01/04/NLP/Course/%E4%BA%8B%E4%BB%B6%E6%8A%BD%E5%8F%96_%E9%99%88%E7%8E%89%E5%8D%9A/chen_47.png" alt="chen_ee"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pyplot/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pyplot/" itemprop="url">pyplot</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T12:10:06+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pyplot"><a href="#pyplot" class="headerlink" title="pyplot"></a>pyplot</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>

<p>使用该库绘制图形</p>
<h2 id="2-矩阵-数组的可视化"><a href="#2-矩阵-数组的可视化" class="headerlink" title="2. 矩阵/数组的可视化"></a>2. 矩阵/数组的可视化</h2><p><code>matshow()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">samplemat</span><span class="params">(dims)</span>:</span></span><br><span class="line">    <span class="string">"""Make a matrix with all zeros and increasing elements on the diagonal"""</span></span><br><span class="line">    aa = np.zeros(dims)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(min(dims)):</span><br><span class="line">        aa[i, i] = i</span><br><span class="line">    <span class="keyword">return</span> aa</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Display matrix</span></span><br><span class="line">plt.matshow(samplemat((<span class="number">15</span>, <span class="number">15</span>)))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/numpy(%E4%B8%80)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/numpy(%E4%B8%80)/" itemprop="url">numpy 系列一</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:37+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Numpy-基础"><a href="#Numpy-基础" class="headerlink" title="Numpy 基础"></a>Numpy 基础</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="1-函数"><a href="#1-函数" class="headerlink" title="1. 函数"></a>1. 函数</h2><h3 id="1-1-生成序列"><a href="#1-1-生成序列" class="headerlink" title="1.1 生成序列"></a>1.1 生成序列</h3><ul>
<li><code>range()</code> 可生成一定 step 的序列，但只能整数</li>
<li><code>arange()</code> 可生成一定 step 的序列，可以是浮点数</li>
</ul>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><h2 id="2-np-eye-与-np-identity"><a href="#2-np-eye-与-np-identity" class="headerlink" title="2. np.eye() 与 np.identity()"></a>2. <code>np.eye()</code> 与 <code>np.identity()</code></h2><p><code>numpy.eye(N, M=None, k=0, dtype=&lt;class &#39;float&#39;&gt;, order=&#39;C&#39;)[source]</code></p>
<p>可以生成2-D的对角矩阵（不一定对角），零可以被设置放置在任意地方；</p>
<h3 id="2-1-Example"><a href="#2-1-Example" class="headerlink" title="2.1 Example"></a>2.1 Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">2</span>, dtype=int)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>, k=<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 常用的可以对数组进行 one-hot 编码</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>)[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]]</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<p><code>np.identity(n, dtype=None)</code></p>
<p>仅能生成对角方阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.identity(<span class="number">3</span>)</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="2-2-numpy-ndarray-flatten"><a href="#2-2-numpy-ndarray-flatten" class="headerlink" title="2.2 numpy.ndarray.flatten"></a>2.2 <code>numpy.ndarray.flatten</code></h3><p><code>ndarray.flatten(order=&#39;C&#39;)</code> 返回一个变换为1-D的副本</p>
<ul>
<li>paramenter: ‘C’- 行；’F’ - 列；’C’是默认的；</li>
</ul>
<p>Example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.flatten()</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.flatten(<span class="string">'F'</span>)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>

<h3 id="2-3-numpy-argmax"><a href="#2-3-numpy-argmax" class="headerlink" title="2.3 numpy.argmax"></a>2.3 <code>numpy.argmax</code></h3><p><code>numpy.argmax(a, axis=None, out=None)</code> 找出 ndarray 中最大值对应的索引</p>
<p>Parameters:</p>
<ul>
<li>axis: 默认是将 array flatten，取其最大值对应的索引，当指定 axis = 0时，每一列的最大值 index；当 axis = 1时，每一行的最大值 index</li>
</ul>
<p>Examples：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = np.arange(<span class="number">6</span>).reshape(<span class="number">2</span>,<span class="number">3</span>) + <span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>],</span><br><span class="line">       [<span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(a)</span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class="number">0</span>)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.argmax(a, axis=<span class="number">1</span>)</span><br><span class="line">array([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/numpy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/numpy/" itemprop="url">numpy</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:37+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Numpy-学习"><a href="#Numpy-学习" class="headerlink" title="Numpy 学习"></a>Numpy 学习</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure>

<h2 id="1-函数"><a href="#1-函数" class="headerlink" title="1. 函数"></a>1. 函数</h2><h3 id="1-1-生成序列"><a href="#1-1-生成序列" class="headerlink" title="1.1 生成序列"></a>1.1 生成序列</h3><ul>
<li><code>range()</code> 可生成一定 step 的序列，但只能整数</li>
<li><code>arange()</code> 可生成一定 step 的序列，可以是浮点数</li>
</ul>
<h2 id="Tricks"><a href="#Tricks" class="headerlink" title="Tricks"></a>Tricks</h2><h2 id="2-np-eye-与-np-identity"><a href="#2-np-eye-与-np-identity" class="headerlink" title="2. np.eye() 与 np.identity()"></a>2. <code>np.eye()</code> 与 <code>np.identity()</code></h2><p><code>numpy.eye(N, M=None, k=0, dtype=&lt;class &#39;float&#39;&gt;, order=&#39;C&#39;)[source]</code></p>
<p>可以生成2-D的对角矩阵（不一定对角），零可以被设置放置在任意地方；</p>
<h3 id="2-1-Example"><a href="#2-1-Example" class="headerlink" title="2.1 Example"></a>2.1 Example</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">2</span>, dtype=int)</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>, k=<span class="number">1</span>)</span><br><span class="line">array([[<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="comment"># 常用的可以对数组进行 one-hot 编码</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.eye(<span class="number">3</span>)[[<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>]]</span><br><span class="line">array([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>

<p><code>np.identity(n, dtype=None)</code></p>
<p>仅能生成对角方阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.identity(<span class="number">3</span>)</span><br><span class="line">array([[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">       [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/numpy%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/numpy%E4%B9%8B%E7%BB%B4%E5%BA%A6%E5%8F%98%E6%8D%A2/" itemprop="url">numpy 之 维度变换</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:37+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="numpy-之维度变换"><a href="#numpy-之维度变换" class="headerlink" title="numpy 之维度变换"></a>numpy 之维度变换</h1><h2 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h2><p><code>numpy</code> 中关于维度变换函数有很多：<code>reshape</code>、<code>resize</code>、<code>swapaxes</code>、<code>flatten</code>、<code>transpose</code>等，下面有关于他们一一讲解。</p>
<h2 id="resize-与-reshape-函数"><a href="#resize-与-reshape-函数" class="headerlink" title="resize() 与 reshape() 函数"></a><code>resize()</code> 与 <code>reshape()</code> 函数</h2><p><code>resize()</code> 与 <code>reshape()</code> 功能相同，主要区别在于：</p>
<ul>
<li><code>resize()</code> 会修改原数组</li>
<li><code>reshape()</code> 不会修改原数组</li>
</ul>
<h2 id="swapaxes-与-transpose-函数"><a href="#swapaxes-与-transpose-函数" class="headerlink" title="swapaxes() 与 transpose() 函数"></a><code>swapaxes()</code> 与 <code>transpose()</code> 函数</h2><ul>
<li><code>swapaxes()</code> 将原数组中的两个维度进行替换，并不改变原数组</li>
<li><code>transpose()</code> 可以进行多个维度的变换，类比于矩阵的转置功能，将原来<code>(i,j,k)</code>上的数据变换到<code>(i,k,j)</code>上来</li>
</ul>
<h2 id="flatten-函数"><a href="#flatten-函数" class="headerlink" title="flatten() 函数"></a><code>flatten()</code> 函数</h2><ul>
<li><code>flatten()</code> 主要对数组进行降维，返回一维数组</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/qq1483661204/article/details/70543952" target="_blank" rel="noopener">https://blog.csdn.net/qq1483661204/article/details/70543952</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pytorch/" itemprop="url">pytorch</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:34+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h1><p><em>参考：《深度学习入门之PyTorch》</em></p>
<h2 id="0-基本数据类型"><a href="#0-基本数据类型" class="headerlink" title="0. 基本数据类型"></a>0. 基本数据类型</h2><h3 id="0-1-数据类型"><a href="#0-1-数据类型" class="headerlink" title="0.1 数据类型"></a>0.1 数据类型</h3><p><code>torch.Tensor</code> 默认是 <code>torch.FloatTensor</code></p>
<table>
<thead>
<tr>
<th>Data tyoe</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody><tr>
<td>32-bit floating point</td>
<td>torch.FloatTensor</td>
<td>torch.cuda.FloatTensor</td>
</tr>
<tr>
<td>64-bit floating point</td>
<td>torch.DoubleTensor</td>
<td>torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>N/A</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td>torch.ByteTensor</td>
<td>torch.cuda.ByteTensor</td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td>torch.IntTensor</td>
<td>torch.cuda.IntTensor</td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td>torch.LongTensor</td>
<td>torch.cuda.LongTensor</td>
</tr>
</tbody></table>
<h3 id="0-2-对-torch-Tensor-的操作方法"><a href="#0-2-对-torch-Tensor-的操作方法" class="headerlink" title="0.2 对 torch.Tensor 的操作方法"></a>0.2 对 <code>torch.Tensor</code> 的操作方法</h3><ul>
<li><code>tensor</code>可以由python中的 <code>list</code> 或序列创建</li>
<li>也可以用python中的切片与索引来修改 <code>tensor</code> 中的内容</li>
<li>会改变 <code>tensor</code> 的函数操作会带有下划线表示，例如：<code>abs_()</code>等</li>
</ul>
<p><code>torch.max()</code> 函数用于选出 tensor 中的最大值，用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">torch.max(a)    <span class="comment"># 返回 a 中最大的元素</span></span><br><span class="line">torch.max(a)[<span class="number">1</span>] <span class="comment"># 返回 a 中最大的元素的索引</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">0</span>) <span class="comment"># 返回 a 中每一列最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">0</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一列最大的元素的索引 index</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">1</span>) <span class="comment"># 返回 a 中每一行最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一行最大的元素的索引 index</span></span><br></pre></td></tr></table></figure>

<h3 id="0-3-变换数据维度"><a href="#0-3-变换数据维度" class="headerlink" title="0.3 变换数据维度"></a>0.3 变换数据维度</h3><p>这里涉及到的总共有 3 种方法：</p>
<p><em>参考：<a href="https://zhuanlan.zhihu.com/p/76583143" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76583143</a></em></p>
<ul>
<li><code>Tensor.permute(a, b, c...)</code> 可以直接对高纬度矩阵进行转置操作</li>
<li><code>torch.transpose()</code> 与 <code>permute()</code>作用相同，但只能操作两个维度</li>
<li><code>Tensor.view()</code>; <code>view()</code> 仅能作用在连续的内存中，即将连续的内存变换成所需要的维度，如果在调用了 <code>transpose()</code> 或 <code>permute()</code> 则可导致内存不连续，需要使用 <code>contiguous()</code>返回一个连续的内存拷贝；</li>
<li><code>torch.reshape()</code> <em>version &gt;=0.4</em>, <code>== tensor.contiguous().view()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a=np.array([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line">unpermuted=torch.tensor(a)</span><br><span class="line">print(unpermuted.size())              <span class="comment">#  ——&gt;  torch.Size([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">permuted=unpermuted.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">print(permuted.size())                <span class="comment">#  ——&gt;  torch.Size([3, 1, 2])</span></span><br><span class="line">print(permuted.is_contiguous())       <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">view_test = unpermuted.view(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">print(view_test.size())               <span class="comment">#  ——&gt;  torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>, <span class="number">2</span>)     <span class="comment">#  --&gt;  使用 -1 进行自动推导，包含两个维度</span></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>)        <span class="comment">#  --&gt;  只包含一个维度</span></span><br></pre></td></tr></table></figure>

<h3 id="0-4-数据拼接"><a href="#0-4-数据拼接" class="headerlink" title="0.4 数据拼接"></a>0.4 数据拼接</h3><ul>
<li><code>torch.cat(tensors, dim=0, out=None) → Tensor</code>，在指定维度上进行数据的拼接，如当<code>dim=0</code>时，在第0维度进行扩展</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>,</span><br><span class="line">         <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="0-5-数据压栈"><a href="#0-5-数据压栈" class="headerlink" title="0.5 数据压栈"></a>0.5 数据压栈</h3><ul>
<li><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></li>
</ul>
<p><code>torch.stack()</code> 要求不同的 <code>tensor</code> 之间是相同的维度，它的作用与转置的作用类似，不同的是 <code>torch.stack()</code> 作用于不同的 <code>tensor</code>，通过指定 <code>dim</code> 去除相应维度的数据进行组合成新的维度，也就是 <code>stack</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.IntTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b = torch.IntTensor([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">d0 = torch.stack([a, b], dim=<span class="number">0</span>)</span><br><span class="line">d1 = torch.stack([a, b], dim=<span class="number">1</span>)</span><br><span class="line">d2 = torch.stack([a, b], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="0-6-数据压缩"><a href="#0-6-数据压缩" class="headerlink" title="0.6 数据压缩"></a>0.6 数据压缩</h3><ul>
<li><code>tensor.squeeze()</code></li>
<li><code>tensor.unsqueeze()</code></li>
</ul>
<p>压缩的维度为元素个数为 1 的维度，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">a.size()                <span class="comment"># [2, 1]</span></span><br><span class="line">a.squeeze().size()      <span class="comment"># [2]</span></span><br></pre></td></tr></table></figure>

<h3 id="0-7-torch-上的一些数学运算：-torch-bmm-torch-mm-torch-matmul-torch-mul"><a href="#0-7-torch-上的一些数学运算：-torch-bmm-torch-mm-torch-matmul-torch-mul" class="headerlink" title="0.7 torch 上的一些数学运算： torch.bmm(),torch.mm(),torch.matmul(),torch.mul()"></a>0.7 <code>torch</code> 上的一些数学运算： <code>torch.bmm(),torch.mm(),torch.matmul(),torch.mul()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵之间的相乘，叉乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mm(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.matmul(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 矩阵的 batch 叉乘运算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = torch.bmm(batch1, batch2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.size()</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 矩阵各元素之间的相乘，是对应位之间的相乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(mat1, mat2)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>torch.sum(input, dim, out=None)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x.sum(<span class="number">0</span>)) <span class="comment">#按列求和</span></span><br><span class="line">print(x.sum(<span class="number">1</span>)) <span class="comment">#按行求和</span></span><br><span class="line">print(torch.sum(x))   <span class="comment">#按列求和</span></span><br><span class="line">print(torch.sum(x, <span class="number">0</span>))<span class="comment">#按列求和</span></span><br><span class="line">print(torch.sum(x, <span class="number">1</span>))<span class="comment">#按行求和</span></span><br></pre></td></tr></table></figure>

<h2 id="1-Variable-变量"><a href="#1-Variable-变量" class="headerlink" title="1. Variable(变量)"></a>1. Variable(变量)</h2><blockquote>
<p>Variable 是神经网络计算图里的概念，提供了自动求导的功能。Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。</p>
</blockquote>
<p>位置： <code>torch.autograd.Variable</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">1</span>]))</span><br><span class="line">w = Variable(torch.Tensor([<span class="number">2</span>]))</span><br><span class="line">b = Variable(torch.Tensor([<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">y = w * x + b</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">pritn(x.grad)</span><br></pre></td></tr></table></figure>

<h3 id="1-1-Parameters"><a href="#1-1-Parameters" class="headerlink" title="1.1 Parameters"></a>1.1 Parameters</h3><p><code>class torch.nn.Parameter()</code></p>
<p><code>Variable</code> 的一种，常被用于模块参数 <code>module parameter</code></p>
<p><code>Variable</code> 已经被废除了。</p>
<p><code>parameter</code> 是类型转换函数，将不可训练的类型<code>Tensor</code>转换成可训练的类型<code>parameter</code>，并将这个<code>parameter</code>绑定到这个 <code>module</code> 里面。（<code>net.parameter()</code> 中就有这个绑定的 <code>parameter</code>，参数优化的时候是可以进行优化的）</p>
<p><code>Variable</code> 与 <code>Parameter</code>  的不同：</p>
<ul>
<li><code>Parameters</code> 是 <code>Variable</code> 的子类。 当把 <code>Parameters</code> 赋值给 <code>Modules</code>的时候，会被自动加到 <code>Module</code> 的参数列表中（会出现在 <code>parameters()</code> 迭代器中）。将 <code>Variable</code> 赋值给 <code>Module</code> 属性则不会有这样的影响。</li>
<li><code>Parameter</code> 不能被 <code>volatile</code>，且默认 <code>requires_grad=True</code>，<code>Variable</code>默认<code>requires_grad=False</code></li>
</ul>
<h2 id="2-Dataset-数据集"><a href="#2-Dataset-数据集" class="headerlink" title="2. Dataset(数据集)"></a>2. Dataset(数据集)</h2><h2 id="3-激励函数"><a href="#3-激励函数" class="headerlink" title="3. 激励函数"></a>3. 激励函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</span><br><span class="line">y_relu = torch.relu(x)</span><br><span class="line">print(y_relu)</span><br><span class="line">y_sigmoid = torch.sigmoid(x)</span><br><span class="line">print(y_sigmoid)</span><br><span class="line">y_tanh = torch.tanh(x)</span><br><span class="line">print(y_tanh)</span><br><span class="line">y_softplus = F.softplus(x)</span><br><span class="line">print(y_softplus)</span><br></pre></td></tr></table></figure>

<h2 id="4-Commonly-used-function"><a href="#4-Commonly-used-function" class="headerlink" title="4. Commonly used function"></a>4. Commonly used function</h2><ul>
<li><code>torch.view()</code> 类似于 <code>numpy.reshape()</code> 但不同的是，<code>view()</code> 函数并不进行变量内存的复制，而只是在原来的内存区域进行操作。</li>
<li><code>torch.Tensor()</code></li>
<li><code>torch.LongTensor()</code></li>
<li><code>torch.optim.SGD()</code></li>
</ul>
<h2 id="5-Containers（容器）-torch-nn-Module"><a href="#5-Containers（容器）-torch-nn-Module" class="headerlink" title="5. Containers（容器） torch.nn Module"></a>5. Containers（容器） <code>torch.nn</code> Module</h2><h3 id="5-1-nn-Linear-in-features-out-features-bias-True"><a href="#5-1-nn-Linear-in-features-out-features-bias-True" class="headerlink" title="5.1 nn.Linear(in_features, out_features, bias=True)"></a>5.1 <code>nn.Linear(in_features, out_features, bias=True)</code></h3><p>$$y=xA^{T}+b$$</p>
<h4 id="5-1-1-Parameters"><a href="#5-1-1-Parameters" class="headerlink" title="5.1.1 Parameters"></a>5.1.1 Parameters</h4><ul>
<li><code>in_features</code> 每个输入样本的大小</li>
<li><code>out_features</code> 每个输出样本的大小</li>
</ul>
<h4 id="5-1-2-Shape"><a href="#5-1-2-Shape" class="headerlink" title="5.1.2 Shape"></a>5.1.2 Shape</h4><ul>
<li><p><code>input:</code><br>$$(N, *, H_{in}), 其中H_{in} = in_features$$</p>
</li>
<li><p><code>output:</code><br>$$(N, *, H_{out}), 其中H_{out} = out_features$$</p>
</li>
</ul>
<h4 id="5-1-3-Example"><a href="#5-1-3-Example" class="headerlink" title="5.1.3 Example"></a>5.1.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure>

<h3 id="5-2-nn-Dropout-p-0-5-inplace-False"><a href="#5-2-nn-Dropout-p-0-5-inplace-False" class="headerlink" title="5.2 nn.Dropout(p=0.5, inplace=False)"></a>5.2 <code>nn.Dropout(p=0.5, inplace=False)</code></h3><ul>
<li><code>p</code> 将元素置0的概率</li>
<li><code>inplace</code> 若设置为 True，会在原地进行操作</li>
</ul>
<h3 id="5-3-nn-ReLU-inplace-False"><a href="#5-3-nn-ReLU-inplace-False" class="headerlink" title="5.3 nn.ReLU(inplace=False)"></a>5.3 <code>nn.ReLU(inplace=False)</code></h3><p>$$<br>y=\begin{cases}<br>0, \quad x\leq0 \\<br>x, \quad x&gt;0<br>\end{cases}<br>$$</p>
<h3 id="5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False"><a href="#5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False" class="headerlink" title="5.4 nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)"></a>5.4 <code>nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)</code></h3><p>保存了固定字典和大小的简单查找表，该模块保存，这里只是初始化的一些向量，后续还需要进行学习和修改</p>
<ul>
<li><code>num_embeddings</code> 嵌入字典的大小</li>
<li><code>embeddings_dim</code> 每个嵌入向量的大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;<span class="string">'hello'</span>: <span class="number">0</span> ,<span class="string">'world'</span>: <span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">hello_idx = torch.LongTensor([word_to_idx[<span class="string">'hello'</span>]])</span><br><span class="line">hello_variable = Variable(hello_idx)</span><br><span class="line">hello_embed = embeds(hello_variable)</span><br><span class="line">print(hello_embed)</span><br><span class="line"></span><br><span class="line">embeds.weight.data = torch.ones(<span class="number">2</span>, <span class="number">5</span>)   <span class="comment"># embeddings 的 weight 可以设置 </span></span><br><span class="line">print(embeds.weight)</span><br></pre></td></tr></table></figure>

<h3 id="5-5-nn-Sequential-args"><a href="#5-5-nn-Sequential-args" class="headerlink" title="5.5 nn.Sequential(*args)"></a>5.5 <code>nn.Sequential(*args)</code></h3><p>一个时序容器。 <code>Modules</code> 会以他们传入的顺序被添加到容器中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu2'</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>

<h3 id="5-6-nn-CrossEntropyLoss"><a href="#5-6-nn-CrossEntropyLoss" class="headerlink" title="5.6 nn.CrossEntropyLoss"></a>5.6 <code>nn.CrossEntropyLoss</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=<span class="number">-100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">'mean'</span>)</span><br></pre></td></tr></table></figure>

<p>该函数相当于组合了 <code>nn.LogSoftmax()</code> 和 <code>nn.NLLLoss()</code></p>
<p>$$loss(x,class) = -log{\frac{exp(x[class])}{\sum_{j}exp(x[j])}}$$</p>
<p>等于（当 <code>weight</code> 参数非零时）</p>
<p>$$loss(x,class) = weight[class]\left(-x[class] + log(\sum_{j}exp\left(x[j]\right)\right)$$</p>
<p>通过以上公式求交叉熵，应用于多分类问题中的<code>loss function</code></p>
<h4 id="5-6-1-Parameters"><a href="#5-6-1-Parameters" class="headerlink" title="5.6.1 Parameters"></a>5.6.1 Parameters</h4><ul>
<li><code>weight</code>，结合上式，赋值给每个class 的 weight</li>
<li><code>reduction</code> 默认为 <code>mean</code>，可以为 <code>none | mean | sum</code>，是对计算后的 output 值的整合</li>
</ul>
<h4 id="5-6-2-Shape"><a href="#5-6-2-Shape" class="headerlink" title="5.6.2 Shape"></a>5.6.2 Shape</h4><ul>
<li>输入1：<code>Input:(N,C)</code> N 代表batch个数，C 代表 class 的个数；</li>
<li>输入2：<code>Target:(N)</code> 代表对应的类别的序号</li>
</ul>
<h4 id="5-6-3-Example"><a href="#5-6-3-Example" class="headerlink" title="5.6.3 Example"></a>5.6.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 输入 batch:3,class numbers: 5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>) <span class="comment"># 输入 target:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(input, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure>

<h3 id="5-7-nn-RNN"><a href="#5-7-nn-RNN" class="headerlink" title="5.7 nn.RNN()"></a>5.7 <code>nn.RNN()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.RNN(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>可以构造多层的RNN，每一层的计算函数为：</p>
<p>$$h_{t} = tanh\left( W_{ih}x_{t} + b_{ih} + W_{hh}h_{(t-1)} + b_{hh}\right)$$</p>
<h4 id="5-7-1-Parameters"><a href="#5-7-1-Parameters" class="headerlink" title="5.7.1 Parameters"></a>5.7.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>nonlinearity:</code> 非线性函数，可以为 <code>tanh</code> 或 <code>relu</code>，默认为 <code>tanh</code></li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional RNN</li>
</ul>
<h4 id="5-7-2-Shape"><a href="#5-7-2-Shape" class="headerlink" title="5.7.2 Shape"></a>5.7.2 Shape</h4><ul>
<li><p><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</p>
</li>
<li><p><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</p>
</li>
<li><p><code>output</code> shape(seq_len, batch, num_directions * hidden_size)，这里的 output 是最后一层 layer 的 output</p>
</li>
</ul>
<h4 id="5-7-3-Example"><a href="#5-7-3-Example" class="headerlink" title="5.7.3 Example"></a>5.7.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.RNN(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)     <span class="comment"># input_size: 10; hidden_size: 20; num_layers: 2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># seq_len: 5; batch: 3; input_size: 10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>) <span class="comment"># num_layers * num_directions: 2; batch: 3; hidden_size: 20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<p><strong>Notation:</strong></p>
<ol>
<li><code>RNN</code> 的 output：是最后一层（the last hidden_layer）的每个 <code>batch</code> 每个 <code>time_step/seq_len</code> 的output</li>
<li><code>RNN</code> 的 h_n 是每一个隐藏层（hidden_layer）的每个 <code>batch</code> 的 output</li>
</ol>
<h3 id="5-8-nn-LSTM"><a href="#5-8-nn-LSTM" class="headerlink" title="5.8 nn.LSTM()"></a>5.8 <code>nn.LSTM()</code></h3><p>$$<br>\begin{aligned}<br>i_{t} =&amp; \ \sigma\left(W_{ii}x_{t} + b_{ii} + W_{hi}h_{(t-1)} + b_{hi}\right) \<br>f_{t} =&amp; \ \sigma\left(W_{if}x_{t} + b_{if} + W_{hf}h_{(t-1)} + b_{hf}\right) \<br>g_{t} =&amp; \ tanh(W_{ig}x_{t} + b_{ig} + W_{hg}h_{(t-1)} + b_{hg}) \<br>o_{t} =&amp; \ \sigma(W_{io}x_{t} + b_{io} + W_{ho}h_{(t-1)} + b_{ho}) \<br>c_{t} =&amp; \ f_{t} * c_{(t-1)} + i_{t} * g_{t} \<br>h_{t} =&amp; \ o_{t} * tanh(c_{t})<br>\end{aligned}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LSTM(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<h4 id="5-8-1-Parameters"><a href="#5-8-1-Parameters" class="headerlink" title="5.8.1 Parameters"></a>5.8.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-8-2-Shape"><a href="#5-8-2-Shape" class="headerlink" title="5.8.2 Shape"></a>5.8.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
<li><code>c_0:</code> shape(num_layers*num_directions, batch, hidden_size), 初始化的 cell state 每个 batch</li>
</ul>
<h4 id="5-8-3-Example"><a href="#5-8-3-Example" class="headerlink" title="5.8.3 Example"></a>5.8.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.LSTM(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, (hn, cn) = rnn(input, (h0, c0))</span><br></pre></td></tr></table></figure>

<h3 id="5-9-nn-GRU"><a href="#5-9-nn-GRU" class="headerlink" title="5.9 nn.GRU()"></a>5.9 <code>nn.GRU()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.GRU(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>$$<br>\begin{aligned}<br>r_{t} =&amp; \ \sigma(W_{ir}x_{t} + b_{ir} + W_{hr}h_{(t-1)} + b_{hr}) \<br>z_{t} =&amp; \ \sigma(W_{iz}x_{t} + b_{iz} + W_{hz}h_{(t-1)} + b_{hz}) \<br>n_{t} =&amp; \ tanh(W_{in}x_{t} + b_{in} + r_{t}*(W_{hn}h_{(t-1)} + b_{hn})) \<br>h_{t} =&amp; \ (1-z_{t}) * n_{t} + z_{t} * h_{(t-1)}<br>\end{aligned}<br>$$</p>
<h4 id="5-9-1-Parameters"><a href="#5-9-1-Parameters" class="headerlink" title="5.9.1 Parameters"></a>5.9.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-9-2-Shape"><a href="#5-9-2-Shape" class="headerlink" title="5.9.2 Shape"></a>5.9.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
</ul>
<h4 id="5-9-3-Example"><a href="#5-9-3-Example" class="headerlink" title="5.9.3 Example"></a>5.9.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<h2 id="6-torch-optim"><a href="#6-torch-optim" class="headerlink" title="6. torch.optim"></a>6. <code>torch.optim</code></h2><p><code>torch.optim</code> 是一个实现了各种优化算法的库。</p>
<h3 id="6-1-如何使用-optimizer"><a href="#6-1-如何使用-optimizer" class="headerlink" title="6.1 如何使用 optimizer"></a>6.1 如何使用 <code>optimizer</code></h3><p>为了使用 <code>torch.optim</code>，你需要构建一个 optimizer 对象，这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。</p>
<ul>
<li>构建</li>
</ul>
<p>为了构建一个 <code>optimizer</code>，需要传入一个包含优化参数（必须都是 <code>Variable</code> 对象）的 iterable，并设置 optimizer 的参数选项，如：学习率、权重衰减等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, )</span><br></pre></td></tr></table></figure>

<ul>
<li>进行单次优化</li>
</ul>
<p>采用的方法：</p>
<p><code>optimizer.step()</code> 这个方法会更新所有的参数，所有的 optimizer 都实现了这个方法，一旦梯度被 <code>backward()</code> 之类的函数计算好后，就可以调用这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单次优化的常用做法</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<h3 id="6-2-常用函数"><a href="#6-2-常用函数" class="headerlink" title="6.2 常用函数"></a>6.2 常用函数</h3><ul>
<li><code>step(closure)</code> 进行单次优化</li>
<li><code>zero_grad()</code> 清空所有被优化过的 Variable 的梯度</li>
</ul>
<h2 id="7-torch-utils"><a href="#7-torch-utils" class="headerlink" title="7. torch.utils"></a>7. <code>torch.utils</code></h2><h3 id="7-1-torch-utils-data-Dataloader"><a href="#7-1-torch-utils-data-Dataloader" class="headerlink" title="7.1 torch.utils.data.Dataloader"></a>7.1 <code>torch.utils.data.Dataloader</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>主要的几个参数：</p>
<ul>
<li>dataset: 数据集来源，可以是 <code>map-style</code> or <code>iterable-style</code> dataset.</li>
<li>batch_size: 每个 <code>batch</code> 有多少个 samples</li>
<li>shuffle: 是否在每个 <code>epoch</code> 进行 reshuffle</li>
</ul>
<p>python 迭代器构造在数据集上。</p>
<h2 id="8-模型的保存与加载"><a href="#8-模型的保存与加载" class="headerlink" title="8. 模型的保存与加载"></a>8. 模型的保存与加载</h2><p><em>参考：<a href="https://zhuanlan.zhihu.com/p/38056115" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38056115</a></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Approach 1</span></span><br><span class="line"><span class="comment"># 保存整个网络，保存的后缀可以为 .pt/.pth</span></span><br><span class="line">torch.save(net, PATH)</span><br><span class="line"></span><br><span class="line">model_dict = torch.load(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Approach 2</span></span><br><span class="line"><span class="comment"># 保存网络中的参数，</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model_dict = model.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Approach 3: Save model to resume training later</span></span><br><span class="line"><span class="comment"># 用于恢复 training 的过程，除了保存 model， 还需要保存 the state of optimizer,epochs,score等。</span></span><br><span class="line">state = &#123;</span><br><span class="line">    <span class="string">'epoch'</span>: epoch,</span><br><span class="line">    <span class="string">'state_dict'</span>: model.state_dict(),</span><br><span class="line">    <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">torch.save(state, filepath)</span><br><span class="line"></span><br><span class="line">model.load_state_dict(state[<span class="string">'state_dict'</span>])</span><br><span class="line">optimizer.load_state_dict(state[<span class="string">'optimizer'</span>])</span><br></pre></td></tr></table></figure>

<p>对于 approach 2 存在的一些缺点：</p>
<blockquote>
<p>However in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.</p>
</blockquote>
<h2 id="9-参考"><a href="#9-参考" class="headerlink" title="9. 参考"></a>9. 参考</h2><ul>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/" target="_blank" rel="noopener">PyTorch-官方教程</a></li>
<li><a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">pytorch-文档</a>    </li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pytorch(2)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/04/MLDL/pytorch(2)/" itemprop="url">pytorch(2)</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:23+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="pytorch-二"><a href="#pytorch-二" class="headerlink" title="pytorch (二)"></a>pytorch (二)</h1><h2 id="1-Variable-Tensor-numpy-之间的转换"><a href="#1-Variable-Tensor-numpy-之间的转换" class="headerlink" title="1. Variable Tensor numpy 之间的转换"></a>1. <code>Variable</code> <code>Tensor</code> <code>numpy</code> 之间的转换</h2><p><strong>Variable 已经被 depreciate 了！！！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy --&gt; tensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tensor_variable = torch.from_numpy(numpy_variable)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor --&gt; numpy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>numpy_variable = tensor_variable.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># numpy -- &gt; Tensor --&gt; Variavle</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>v = Variable(torch.from_numpy(numpy_variable))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable --&gt; numpy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>numpy_variable = v.data.numpy()</span><br></pre></td></tr></table></figure>

<h2 id="2-tensor-new"><a href="#2-tensor-new" class="headerlink" title="2. tensor.new()"></a>2. <code>tensor.new()</code></h2><p><em><a href="https://pytorch.org/docs/0.3.1/tensors.html?highlight=new#torch.Tensor.new" target="_blank" rel="noopener">source</a></em></p>
<p><code>new(*args, **kwargs)</code></p>
<p>Constructs a new tensor of the same data type as self tensor.</p>
<blockquote>
<p>For CUDA tensors, this method will create new tensor on the same device as this tensor.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = (input.data.new([word2ix[w]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br></pre></td></tr></table></figure>

<h2 id="3-cuda-上-tensor-的定义"><a href="#3-cuda-上-tensor-的定义" class="headerlink" title="3. cuda 上 tensor 的定义"></a>3. <code>cuda</code> 上 <code>tensor</code> 的定义</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.one(<span class="number">1000</span>, <span class="number">1000</span>, <span class="number">3</span>).cuda()     <span class="comment"># 在 cpu 上定义，将数据转移到 cuda 上</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros().cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在某一GPU设备上定义</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>cuda1 = torch.deivce(<span class="string">'cuda:1'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn((<span class="number">1000</span>, <span class="number">1000</span>, <span class="number">1000</span>), device=cuda1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接在 GPU上定义数据，减少cpu的损耗</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.FloatTensor(batch_size, self.hidden_dim, self.height, self.width).fill_(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/02/MLDL/Ipdb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/01/02/MLDL/Ipdb/" itemprop="url">Ipdb</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-02T12:05:05+08:00">
                2020-01-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="IPDB-Ipython-Debugger"><a href="#IPDB-Ipython-Debugger" class="headerlink" title="IPDB (Ipython Debugger)"></a>IPDB (Ipython Debugger)</h1><p>IPDB和GDB类似，是一款集成了<code>Ipython</code>的Python代码命令行调试工具。</p>
<h2 id="1-集成到源代码中使用"><a href="#1-集成到源代码中使用" class="headerlink" title="1. 集成到源代码中使用"></a>1. 集成到源代码中使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"></span><br><span class="line">x = <span class="number">10</span></span><br><span class="line">ipdb.set_trace() <span class="comment"># 执行到此，程序停止，展开Ipython环境</span></span><br><span class="line">y = <span class="number">20</span></span><br></pre></td></tr></table></figure>

<h2 id="2-交互式使用"><a href="#2-交互式使用" class="headerlink" title="2. 交互式使用"></a>2. 交互式使用</h2><p><code>python -m ipdb code.py</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/">&lt;i class=&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="extend next" rel="next" href="/page/8/">&lt;i class=&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="wanncy" />
            
              <p class="site-author-name" itemprop="name">wanncy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">214</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">58</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">56</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wangda1" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wanncy</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        




  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66562237";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
