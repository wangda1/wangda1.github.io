<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="MLDL," />










<meta name="description" content="PyTorch参考：《深度学习入门之PyTorch》 0. 基本数据类型0.1 数据类型torch.Tensor 默认是 torch.FloatTensor    Data tyoe CPU tensor GPU tensor    32-bit floating point torch.FloatTensor torch.cuda.FloatTensor   64-bit floating">
<meta name="keywords" content="MLDL">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;2020&#x2F;01&#x2F;04&#x2F;MLDL&#x2F;pytorch&#x2F;index.html">
<meta property="og:site_name" content="笔记记录">
<meta property="og:description" content="PyTorch参考：《深度学习入门之PyTorch》 0. 基本数据类型0.1 数据类型torch.Tensor 默认是 torch.FloatTensor    Data tyoe CPU tensor GPU tensor    32-bit floating point torch.FloatTensor torch.cuda.FloatTensor   64-bit floating">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2020-05-07T14:37:38.348Z">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/01/04/MLDL/pytorch/"/>





  <title>pytorch | 笔记记录</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">笔记记录</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" target="_blank" rel="noopener" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/04/MLDL/pytorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wanncy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="笔记记录">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">pytorch</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-04T11:55:34+08:00">
                2020-01-04
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MLDL/" itemprop="url" rel="index">
                    <span itemprop="name">MLDL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="PyTorch"><a href="#PyTorch" class="headerlink" title="PyTorch"></a>PyTorch</h1><p><em>参考：《深度学习入门之PyTorch》</em></p>
<h2 id="0-基本数据类型"><a href="#0-基本数据类型" class="headerlink" title="0. 基本数据类型"></a>0. 基本数据类型</h2><h3 id="0-1-数据类型"><a href="#0-1-数据类型" class="headerlink" title="0.1 数据类型"></a>0.1 数据类型</h3><p><code>torch.Tensor</code> 默认是 <code>torch.FloatTensor</code></p>
<table>
<thead>
<tr>
<th>Data tyoe</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody><tr>
<td>32-bit floating point</td>
<td>torch.FloatTensor</td>
<td>torch.cuda.FloatTensor</td>
</tr>
<tr>
<td>64-bit floating point</td>
<td>torch.DoubleTensor</td>
<td>torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>N/A</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td>torch.ByteTensor</td>
<td>torch.cuda.ByteTensor</td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td>torch.IntTensor</td>
<td>torch.cuda.IntTensor</td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td>torch.LongTensor</td>
<td>torch.cuda.LongTensor</td>
</tr>
</tbody></table>
<h3 id="0-2-对-torch-Tensor-的操作方法"><a href="#0-2-对-torch-Tensor-的操作方法" class="headerlink" title="0.2 对 torch.Tensor 的操作方法"></a>0.2 对 <code>torch.Tensor</code> 的操作方法</h3><ul>
<li><code>tensor</code>可以由python中的 <code>list</code> 或序列创建</li>
<li>也可以用python中的切片与索引来修改 <code>tensor</code> 中的内容</li>
<li>会改变 <code>tensor</code> 的函数操作会带有下划线表示，例如：<code>abs_()</code>等</li>
</ul>
<p><code>torch.max()</code> 函数用于选出 tensor 中的最大值，用法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">torch.max(a)    <span class="comment"># 返回 a 中最大的元素</span></span><br><span class="line">torch.max(a)[<span class="number">1</span>] <span class="comment"># 返回 a 中最大的元素的索引</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">0</span>) <span class="comment"># 返回 a 中每一列最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">0</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一列最大的元素的索引 index</span></span><br><span class="line"></span><br><span class="line">torch.max(a, <span class="number">1</span>) <span class="comment"># 返回 a 中每一行最大的元素</span></span><br><span class="line">torch.max(a, <span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 返回 a 中每一行最大的元素的索引 index</span></span><br></pre></td></tr></table></figure>

<h3 id="0-3-变换数据维度"><a href="#0-3-变换数据维度" class="headerlink" title="0.3 变换数据维度"></a>0.3 变换数据维度</h3><p>这里涉及到的总共有 3 种方法：</p>
<p><em>参考：<a href="https://zhuanlan.zhihu.com/p/76583143" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76583143</a></em></p>
<ul>
<li><code>Tensor.permute(a, b, c...)</code> 可以直接对高纬度矩阵进行转置操作</li>
<li><code>torch.transpose()</code> 与 <code>permute()</code>作用相同，但只能操作两个维度</li>
<li><code>Tensor.view()</code>; <code>view()</code> 仅能作用在连续的内存中，即将连续的内存变换成所需要的维度，如果在调用了 <code>transpose()</code> 或 <code>permute()</code> 则可导致内存不连续，需要使用 <code>contiguous()</code>返回一个连续的内存拷贝；</li>
<li><code>torch.reshape()</code> <em>version &gt;=0.4</em>, <code>== tensor.contiguous().view()</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a=np.array([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line">unpermuted=torch.tensor(a)</span><br><span class="line">print(unpermuted.size())              <span class="comment">#  ——&gt;  torch.Size([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">permuted=unpermuted.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">print(permuted.size())                <span class="comment">#  ——&gt;  torch.Size([3, 1, 2])</span></span><br><span class="line">print(permuted.is_contiguous())       <span class="comment"># False</span></span><br><span class="line"></span><br><span class="line">view_test = unpermuted.view(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">print(view_test.size())               <span class="comment">#  ——&gt;  torch.Size([1, 3, 2])</span></span><br><span class="line"></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>, <span class="number">2</span>)     <span class="comment">#  --&gt;  使用 -1 进行自动推导，包含两个维度</span></span><br><span class="line">view_test = view_test.view(<span class="number">-1</span>)        <span class="comment">#  --&gt;  只包含一个维度</span></span><br></pre></td></tr></table></figure>

<h3 id="0-4-数据拼接"><a href="#0-4-数据拼接" class="headerlink" title="0.4 数据拼接"></a>0.4 数据拼接</h3><ul>
<li><code>torch.cat(tensors, dim=0, out=None) → Tensor</code>，在指定维度上进行数据的拼接，如当<code>dim=0</code>时，在第0维度进行扩展</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>, <span class="number">-1.0969</span>, <span class="number">-0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         <span class="number">-1.0969</span>, <span class="number">-0.4614</span>],</span><br><span class="line">        [<span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>, <span class="number">-0.5790</span>,  <span class="number">0.1497</span>, <span class="number">-0.1034</span>,</span><br><span class="line">         <span class="number">-0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>

<h3 id="0-5-数据压栈"><a href="#0-5-数据压栈" class="headerlink" title="0.5 数据压栈"></a>0.5 数据压栈</h3><ul>
<li><code>torch.stack(tensors, dim=0, out=None) → Tensor</code></li>
</ul>
<p><code>torch.stack()</code> 要求不同的 <code>tensor</code> 之间是相同的维度，它的作用与转置的作用类似，不同的是 <code>torch.stack()</code> 作用于不同的 <code>tensor</code>，通过指定 <code>dim</code> 去除相应维度的数据进行组合成新的维度，也就是 <code>stack</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.IntTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b = torch.IntTensor([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">d0 = torch.stack([a, b], dim=<span class="number">0</span>)</span><br><span class="line">d1 = torch.stack([a, b], dim=<span class="number">1</span>)</span><br><span class="line">d2 = torch.stack([a, b], dim=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h3 id="0-6-数据压缩"><a href="#0-6-数据压缩" class="headerlink" title="0.6 数据压缩"></a>0.6 数据压缩</h3><ul>
<li><code>tensor.squeeze()</code></li>
<li><code>tensor.unsqueeze()</code></li>
</ul>
<p>压缩的维度为元素个数为 1 的维度，如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">a.size()                <span class="comment"># [2, 1]</span></span><br><span class="line">a.squeeze().size()      <span class="comment"># [2]</span></span><br></pre></td></tr></table></figure>

<h3 id="0-7-torch-上的一些数学运算：-torch-bmm-torch-mm-torch-matmul-torch-mul"><a href="#0-7-torch-上的一些数学运算：-torch-bmm-torch-mm-torch-matmul-torch-mul" class="headerlink" title="0.7 torch 上的一些数学运算： torch.bmm(),torch.mm(),torch.matmul(),torch.mul()"></a>0.7 <code>torch</code> 上的一些数学运算： <code>torch.bmm(),torch.mm(),torch.matmul(),torch.mul()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 矩阵之间的相乘，叉乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mm(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.matmul(mat1, mat2, out=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 矩阵的 batch 叉乘运算</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch1 = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch2 = torch.randn(<span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res = torch.bmm(batch1, batch2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.size()</span><br><span class="line">torch.Size([<span class="number">10</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 矩阵各元素之间的相乘，是对应位之间的相乘</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(mat1, mat2)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>torch.sum(input, dim, out=None)</code></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x.sum(<span class="number">0</span>)) <span class="comment">#按列求和</span></span><br><span class="line">print(x.sum(<span class="number">1</span>)) <span class="comment">#按行求和</span></span><br><span class="line">print(torch.sum(x))   <span class="comment">#按列求和</span></span><br><span class="line">print(torch.sum(x, <span class="number">0</span>))<span class="comment">#按列求和</span></span><br><span class="line">print(torch.sum(x, <span class="number">1</span>))<span class="comment">#按行求和</span></span><br></pre></td></tr></table></figure>

<h2 id="1-Variable-变量"><a href="#1-Variable-变量" class="headerlink" title="1. Variable(变量)"></a>1. Variable(变量)</h2><blockquote>
<p>Variable 是神经网络计算图里的概念，提供了自动求导的功能。Variable 和 Tensor 本质上没有区别，不过 Variable 会被放入一个计算图中，然后进行前向传播、反向传播、自动求导。</p>
</blockquote>
<p>位置： <code>torch.autograd.Variable</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">1</span>]))</span><br><span class="line">w = Variable(torch.Tensor([<span class="number">2</span>]))</span><br><span class="line">b = Variable(torch.Tensor([<span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">y = w * x + b</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line">pritn(x.grad)</span><br></pre></td></tr></table></figure>

<h3 id="1-1-Parameters"><a href="#1-1-Parameters" class="headerlink" title="1.1 Parameters"></a>1.1 Parameters</h3><p><code>class torch.nn.Parameter()</code></p>
<p><code>Variable</code> 的一种，常被用于模块参数 <code>module parameter</code></p>
<p><code>Variable</code> 已经被废除了。</p>
<p><code>parameter</code> 是类型转换函数，将不可训练的类型<code>Tensor</code>转换成可训练的类型<code>parameter</code>，并将这个<code>parameter</code>绑定到这个 <code>module</code> 里面。（<code>net.parameter()</code> 中就有这个绑定的 <code>parameter</code>，参数优化的时候是可以进行优化的）</p>
<p><code>Variable</code> 与 <code>Parameter</code>  的不同：</p>
<ul>
<li><code>Parameters</code> 是 <code>Variable</code> 的子类。 当把 <code>Parameters</code> 赋值给 <code>Modules</code>的时候，会被自动加到 <code>Module</code> 的参数列表中（会出现在 <code>parameters()</code> 迭代器中）。将 <code>Variable</code> 赋值给 <code>Module</code> 属性则不会有这样的影响。</li>
<li><code>Parameter</code> 不能被 <code>volatile</code>，且默认 <code>requires_grad=True</code>，<code>Variable</code>默认<code>requires_grad=False</code></li>
</ul>
<h2 id="2-Dataset-数据集"><a href="#2-Dataset-数据集" class="headerlink" title="2. Dataset(数据集)"></a>2. Dataset(数据集)</h2><h2 id="3-激励函数"><a href="#3-激励函数" class="headerlink" title="3. 激励函数"></a>3. 激励函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.linspace(<span class="number">-5</span>, <span class="number">5</span>, <span class="number">200</span>)</span><br><span class="line">y_relu = torch.relu(x)</span><br><span class="line">print(y_relu)</span><br><span class="line">y_sigmoid = torch.sigmoid(x)</span><br><span class="line">print(y_sigmoid)</span><br><span class="line">y_tanh = torch.tanh(x)</span><br><span class="line">print(y_tanh)</span><br><span class="line">y_softplus = F.softplus(x)</span><br><span class="line">print(y_softplus)</span><br></pre></td></tr></table></figure>

<h2 id="4-Commonly-used-function"><a href="#4-Commonly-used-function" class="headerlink" title="4. Commonly used function"></a>4. Commonly used function</h2><ul>
<li><code>torch.view()</code> 类似于 <code>numpy.reshape()</code> 但不同的是，<code>view()</code> 函数并不进行变量内存的复制，而只是在原来的内存区域进行操作。</li>
<li><code>torch.Tensor()</code></li>
<li><code>torch.LongTensor()</code></li>
<li><code>torch.optim.SGD()</code></li>
</ul>
<h2 id="5-Containers（容器）-torch-nn-Module"><a href="#5-Containers（容器）-torch-nn-Module" class="headerlink" title="5. Containers（容器） torch.nn Module"></a>5. Containers（容器） <code>torch.nn</code> Module</h2><h3 id="5-1-nn-Linear-in-features-out-features-bias-True"><a href="#5-1-nn-Linear-in-features-out-features-bias-True" class="headerlink" title="5.1 nn.Linear(in_features, out_features, bias=True)"></a>5.1 <code>nn.Linear(in_features, out_features, bias=True)</code></h3><p>$$y=xA^{T}+b$$</p>
<h4 id="5-1-1-Parameters"><a href="#5-1-1-Parameters" class="headerlink" title="5.1.1 Parameters"></a>5.1.1 Parameters</h4><ul>
<li><code>in_features</code> 每个输入样本的大小</li>
<li><code>out_features</code> 每个输出样本的大小</li>
</ul>
<h4 id="5-1-2-Shape"><a href="#5-1-2-Shape" class="headerlink" title="5.1.2 Shape"></a>5.1.2 Shape</h4><ul>
<li><p><code>input:</code><br>$$(N, *, H_{in}), 其中H_{in} = in_features$$</p>
</li>
<li><p><code>output:</code><br>$$(N, *, H_{out}), 其中H_{out} = out_features$$</p>
</li>
</ul>
<h4 id="5-1-3-Example"><a href="#5-1-3-Example" class="headerlink" title="5.1.3 Example"></a>5.1.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = m(input)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure>

<h3 id="5-2-nn-Dropout-p-0-5-inplace-False"><a href="#5-2-nn-Dropout-p-0-5-inplace-False" class="headerlink" title="5.2 nn.Dropout(p=0.5, inplace=False)"></a>5.2 <code>nn.Dropout(p=0.5, inplace=False)</code></h3><ul>
<li><code>p</code> 将元素置0的概率</li>
<li><code>inplace</code> 若设置为 True，会在原地进行操作</li>
</ul>
<h3 id="5-3-nn-ReLU-inplace-False"><a href="#5-3-nn-ReLU-inplace-False" class="headerlink" title="5.3 nn.ReLU(inplace=False)"></a>5.3 <code>nn.ReLU(inplace=False)</code></h3><p>$$<br>y=\begin{cases}<br>0, \quad x\leq0 \\<br>x, \quad x&gt;0<br>\end{cases}<br>$$</p>
<h3 id="5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False"><a href="#5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False" class="headerlink" title="5.4 nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)"></a>5.4 <code>nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)</code></h3><p>保存了固定字典和大小的简单查找表，该模块保存，这里只是初始化的一些向量，后续还需要进行学习和修改</p>
<ul>
<li><code>num_embeddings</code> 嵌入字典的大小</li>
<li><code>embeddings_dim</code> 每个嵌入向量的大小</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">word_to_idx = &#123;<span class="string">'hello'</span>: <span class="number">0</span> ,<span class="string">'world'</span>: <span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embedding(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line">hello_idx = torch.LongTensor([word_to_idx[<span class="string">'hello'</span>]])</span><br><span class="line">hello_variable = Variable(hello_idx)</span><br><span class="line">hello_embed = embeds(hello_variable)</span><br><span class="line">print(hello_embed)</span><br><span class="line"></span><br><span class="line">embeds.weight.data = torch.ones(<span class="number">2</span>, <span class="number">5</span>)   <span class="comment"># embeddings 的 weight 可以设置 </span></span><br><span class="line">print(embeds.weight)</span><br></pre></td></tr></table></figure>

<h3 id="5-5-nn-Sequential-args"><a href="#5-5-nn-Sequential-args" class="headerlink" title="5.5 nn.Sequential(*args)"></a>5.5 <code>nn.Sequential(*args)</code></h3><p>一个时序容器。 <code>Modules</code> 会以他们传入的顺序被添加到容器中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu1'</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">'conv2'</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">'relu2'</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure>

<h3 id="5-6-nn-CrossEntropyLoss"><a href="#5-6-nn-CrossEntropyLoss" class="headerlink" title="5.6 nn.CrossEntropyLoss"></a>5.6 <code>nn.CrossEntropyLoss</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=<span class="number">-100</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">'mean'</span>)</span><br></pre></td></tr></table></figure>

<p>该函数相当于组合了 <code>nn.LogSoftmax()</code> 和 <code>nn.NLLLoss()</code></p>
<p>$$loss(x,class) = -log{\frac{exp(x[class])}{\sum_{j}exp(x[j])}}$$</p>
<p>等于（当 <code>weight</code> 参数非零时）</p>
<p>$$loss(x,class) = weight[class]\left(-x[class] + log(\sum_{j}exp\left(x[j]\right)\right)$$</p>
<p>通过以上公式求交叉熵，应用于多分类问题中的<code>loss function</code></p>
<h4 id="5-6-1-Parameters"><a href="#5-6-1-Parameters" class="headerlink" title="5.6.1 Parameters"></a>5.6.1 Parameters</h4><ul>
<li><code>weight</code>，结合上式，赋值给每个class 的 weight</li>
<li><code>reduction</code> 默认为 <code>mean</code>，可以为 <code>none | mean | sum</code>，是对计算后的 output 值的整合</li>
</ul>
<h4 id="5-6-2-Shape"><a href="#5-6-2-Shape" class="headerlink" title="5.6.2 Shape"></a>5.6.2 Shape</h4><ul>
<li>输入1：<code>Input:(N,C)</code> N 代表batch个数，C 代表 class 的个数；</li>
<li>输入2：<code>Target:(N)</code> 代表对应的类别的序号</li>
</ul>
<h4 id="5-6-3-Example"><a href="#5-6-3-Example" class="headerlink" title="5.6.3 Example"></a>5.6.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss(reduction=<span class="string">'none'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)  <span class="comment"># 输入 batch:3,class numbers: 5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>) <span class="comment"># 输入 target:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(input, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure>

<h3 id="5-7-nn-RNN"><a href="#5-7-nn-RNN" class="headerlink" title="5.7 nn.RNN()"></a>5.7 <code>nn.RNN()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.RNN(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>可以构造多层的RNN，每一层的计算函数为：</p>
<p>$$h_{t} = tanh\left( W_{ih}x_{t} + b_{ih} + W_{hh}h_{(t-1)} + b_{hh}\right)$$</p>
<h4 id="5-7-1-Parameters"><a href="#5-7-1-Parameters" class="headerlink" title="5.7.1 Parameters"></a>5.7.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>nonlinearity:</code> 非线性函数，可以为 <code>tanh</code> 或 <code>relu</code>，默认为 <code>tanh</code></li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional RNN</li>
</ul>
<h4 id="5-7-2-Shape"><a href="#5-7-2-Shape" class="headerlink" title="5.7.2 Shape"></a>5.7.2 Shape</h4><ul>
<li><p><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</p>
</li>
<li><p><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</p>
</li>
<li><p><code>output</code> shape(seq_len, batch, num_directions * hidden_size)，这里的 output 是最后一层 layer 的 output</p>
</li>
</ul>
<h4 id="5-7-3-Example"><a href="#5-7-3-Example" class="headerlink" title="5.7.3 Example"></a>5.7.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.RNN(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)     <span class="comment"># input_size: 10; hidden_size: 20; num_layers: 2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>) <span class="comment"># seq_len: 5; batch: 3; input_size: 10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>) <span class="comment"># num_layers * num_directions: 2; batch: 3; hidden_size: 20</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<p><strong>Notation:</strong></p>
<ol>
<li><code>RNN</code> 的 output：是最后一层（the last hidden_layer）的每个 <code>batch</code> 每个 <code>time_step/seq_len</code> 的output</li>
<li><code>RNN</code> 的 h_n 是每一个隐藏层（hidden_layer）的每个 <code>batch</code> 的 output</li>
</ol>
<h3 id="5-8-nn-LSTM"><a href="#5-8-nn-LSTM" class="headerlink" title="5.8 nn.LSTM()"></a>5.8 <code>nn.LSTM()</code></h3><p>$$<br>\begin{aligned}<br>i_{t} =&amp; \ \sigma\left(W_{ii}x_{t} + b_{ii} + W_{hi}h_{(t-1)} + b_{hi}\right) \<br>f_{t} =&amp; \ \sigma\left(W_{if}x_{t} + b_{if} + W_{hf}h_{(t-1)} + b_{hf}\right) \<br>g_{t} =&amp; \ tanh(W_{ig}x_{t} + b_{ig} + W_{hg}h_{(t-1)} + b_{hg}) \<br>o_{t} =&amp; \ \sigma(W_{io}x_{t} + b_{io} + W_{ho}h_{(t-1)} + b_{ho}) \<br>c_{t} =&amp; \ f_{t} * c_{(t-1)} + i_{t} * g_{t} \<br>h_{t} =&amp; \ o_{t} * tanh(c_{t})<br>\end{aligned}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.LSTM(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<h4 id="5-8-1-Parameters"><a href="#5-8-1-Parameters" class="headerlink" title="5.8.1 Parameters"></a>5.8.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-8-2-Shape"><a href="#5-8-2-Shape" class="headerlink" title="5.8.2 Shape"></a>5.8.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
<li><code>c_0:</code> shape(num_layers*num_directions, batch, hidden_size), 初始化的 cell state 每个 batch</li>
</ul>
<h4 id="5-8-3-Example"><a href="#5-8-3-Example" class="headerlink" title="5.8.3 Example"></a>5.8.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.LSTM(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, (hn, cn) = rnn(input, (h0, c0))</span><br></pre></td></tr></table></figure>

<h3 id="5-9-nn-GRU"><a href="#5-9-nn-GRU" class="headerlink" title="5.9 nn.GRU()"></a>5.9 <code>nn.GRU()</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.GRU(*args, **kwargs)</span><br></pre></td></tr></table></figure>

<p>$$<br>\begin{aligned}<br>r_{t} =&amp; \ \sigma(W_{ir}x_{t} + b_{ir} + W_{hr}h_{(t-1)} + b_{hr}) \<br>z_{t} =&amp; \ \sigma(W_{iz}x_{t} + b_{iz} + W_{hz}h_{(t-1)} + b_{hz}) \<br>n_{t} =&amp; \ tanh(W_{in}x_{t} + b_{in} + r_{t}*(W_{hn}h_{(t-1)} + b_{hn})) \<br>h_{t} =&amp; \ (1-z_{t}) * n_{t} + z_{t} * h_{(t-1)}<br>\end{aligned}<br>$$</p>
<h4 id="5-9-1-Parameters"><a href="#5-9-1-Parameters" class="headerlink" title="5.9.1 Parameters"></a>5.9.1 Parameters</h4><ul>
<li><code>input_size:</code> input 的 feature 数量</li>
<li><code>hidden_size:</code> hidden layer 的feature 数量</li>
<li><code>num_layers:</code> 循环RNN的数量，可以组成 stacked RNN</li>
<li><code>bidirectional:</code> 设置为 <code>True</code>，则为 bidirectional LSTM</li>
</ul>
<h4 id="5-9-2-Shape"><a href="#5-9-2-Shape" class="headerlink" title="5.9.2 Shape"></a>5.9.2 Shape</h4><ul>
<li><code>input:</code> shape(seq_len, batch, input_size)，多少个sequence，每个sequence带有多少个batch，每个batch里的 input_size</li>
<li><code>h_0:</code>  shape(num_layers*num_directions, batch, hidden_size)，这里设置 hidden layer 的参数</li>
</ul>
<h4 id="5-9-3-Example"><a href="#5-9-3-Example" class="headerlink" title="5.9.3 Example"></a>5.9.3 Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rnn = nn.GRU(<span class="number">10</span>, <span class="number">20</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">5</span>, <span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>h0 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">20</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, hn = rnn(input, h0)</span><br></pre></td></tr></table></figure>

<h2 id="6-torch-optim"><a href="#6-torch-optim" class="headerlink" title="6. torch.optim"></a>6. <code>torch.optim</code></h2><p><code>torch.optim</code> 是一个实现了各种优化算法的库。</p>
<h3 id="6-1-如何使用-optimizer"><a href="#6-1-如何使用-optimizer" class="headerlink" title="6.1 如何使用 optimizer"></a>6.1 如何使用 <code>optimizer</code></h3><p>为了使用 <code>torch.optim</code>，你需要构建一个 optimizer 对象，这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。</p>
<ul>
<li>构建</li>
</ul>
<p>为了构建一个 <code>optimizer</code>，需要传入一个包含优化参数（必须都是 <code>Variable</code> 对象）的 iterable，并设置 optimizer 的参数选项，如：学习率、权重衰减等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, )</span><br></pre></td></tr></table></figure>

<ul>
<li>进行单次优化</li>
</ul>
<p>采用的方法：</p>
<p><code>optimizer.step()</code> 这个方法会更新所有的参数，所有的 optimizer 都实现了这个方法，一旦梯度被 <code>backward()</code> 之类的函数计算好后，就可以调用这个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 单次优化的常用做法</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = model(input)</span><br><span class="line">    loss = loss_fn(output, target)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<h3 id="6-2-常用函数"><a href="#6-2-常用函数" class="headerlink" title="6.2 常用函数"></a>6.2 常用函数</h3><ul>
<li><code>step(closure)</code> 进行单次优化</li>
<li><code>zero_grad()</code> 清空所有被优化过的 Variable 的梯度</li>
</ul>
<h2 id="7-torch-utils"><a href="#7-torch-utils" class="headerlink" title="7. torch.utils"></a>7. <code>torch.utils</code></h2><h3 id="7-1-torch-utils-data-Dataloader"><a href="#7-1-torch-utils-data-Dataloader" class="headerlink" title="7.1 torch.utils.data.Dataloader"></a>7.1 <code>torch.utils.data.Dataloader</code></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>,</span><br><span class="line">           batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>,</span><br><span class="line">           pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>,</span><br><span class="line">           worker_init_fn=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>主要的几个参数：</p>
<ul>
<li>dataset: 数据集来源，可以是 <code>map-style</code> or <code>iterable-style</code> dataset.</li>
<li>batch_size: 每个 <code>batch</code> 有多少个 samples</li>
<li>shuffle: 是否在每个 <code>epoch</code> 进行 reshuffle</li>
</ul>
<p>python 迭代器构造在数据集上。</p>
<h2 id="8-模型的保存与加载"><a href="#8-模型的保存与加载" class="headerlink" title="8. 模型的保存与加载"></a>8. 模型的保存与加载</h2><p><em>参考：<a href="https://zhuanlan.zhihu.com/p/38056115" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38056115</a></em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Approach 1</span></span><br><span class="line"><span class="comment"># 保存整个网络，保存的后缀可以为 .pt/.pth</span></span><br><span class="line">torch.save(net, PATH)</span><br><span class="line"></span><br><span class="line">model_dict = torch.load(PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Approach 2</span></span><br><span class="line"><span class="comment"># 保存网络中的参数，</span></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">model_dict = model.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Approach 3: Save model to resume training later</span></span><br><span class="line"><span class="comment"># 用于恢复 training 的过程，除了保存 model， 还需要保存 the state of optimizer,epochs,score等。</span></span><br><span class="line">state = &#123;</span><br><span class="line">    <span class="string">'epoch'</span>: epoch,</span><br><span class="line">    <span class="string">'state_dict'</span>: model.state_dict(),</span><br><span class="line">    <span class="string">'optimizer'</span>: optimizer.state_dict(),</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">torch.save(state, filepath)</span><br><span class="line"></span><br><span class="line">model.load_state_dict(state[<span class="string">'state_dict'</span>])</span><br><span class="line">optimizer.load_state_dict(state[<span class="string">'optimizer'</span>])</span><br></pre></td></tr></table></figure>

<p>对于 approach 2 存在的一些缺点：</p>
<blockquote>
<p>However in this case, the serialized data is bound to the specific classes and the exact directory structure used, so it can break in various ways when used in other projects, or after some serious refactors.</p>
</blockquote>
<h2 id="9-参考"><a href="#9-参考" class="headerlink" title="9. 参考"></a>9. 参考</h2><ul>
<li><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/" target="_blank" rel="noopener">PyTorch-官方教程</a></li>
<li><a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">pytorch-文档</a>    </li>
</ul>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div></div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.png" alt="wanncy 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="wanncy 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/MLDL/" rel="tag"># MLDL</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/01/04/MLDL/pytorch(2)/" rel="next" title="pytorch(2)">
                <i class="fa fa-chevron-left"></i> pytorch(2)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/01/04/MLDL/numpy/" rel="prev" title="numpy">
                numpy <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="wanncy" />
            
              <p class="site-author-name" itemprop="name">wanncy</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">256</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">65</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">63</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/wangda1" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#PyTorch"><span class="nav-number">1.</span> <span class="nav-text">PyTorch</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-基本数据类型"><span class="nav-number">1.1.</span> <span class="nav-text">0. 基本数据类型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0-1-数据类型"><span class="nav-number">1.1.1.</span> <span class="nav-text">0.1 数据类型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-2-对-torch-Tensor-的操作方法"><span class="nav-number">1.1.2.</span> <span class="nav-text">0.2 对 torch.Tensor 的操作方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-3-变换数据维度"><span class="nav-number">1.1.3.</span> <span class="nav-text">0.3 变换数据维度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-4-数据拼接"><span class="nav-number">1.1.4.</span> <span class="nav-text">0.4 数据拼接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-5-数据压栈"><span class="nav-number">1.1.5.</span> <span class="nav-text">0.5 数据压栈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-6-数据压缩"><span class="nav-number">1.1.6.</span> <span class="nav-text">0.6 数据压缩</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#0-7-torch-上的一些数学运算：-torch-bmm-torch-mm-torch-matmul-torch-mul"><span class="nav-number">1.1.7.</span> <span class="nav-text">0.7 torch 上的一些数学运算： torch.bmm(),torch.mm(),torch.matmul(),torch.mul()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Variable-变量"><span class="nav-number">1.2.</span> <span class="nav-text">1. Variable(变量)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Parameters"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.1 Parameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Dataset-数据集"><span class="nav-number">1.3.</span> <span class="nav-text">2. Dataset(数据集)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-激励函数"><span class="nav-number">1.4.</span> <span class="nav-text">3. 激励函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Commonly-used-function"><span class="nav-number">1.5.</span> <span class="nav-text">4. Commonly used function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Containers（容器）-torch-nn-Module"><span class="nav-number">1.6.</span> <span class="nav-text">5. Containers（容器） torch.nn Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-nn-Linear-in-features-out-features-bias-True"><span class="nav-number">1.6.1.</span> <span class="nav-text">5.1 nn.Linear(in_features, out_features, bias=True)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-1-Parameters"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">5.1.1 Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-2-Shape"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">5.1.2 Shape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-3-Example"><span class="nav-number">1.6.1.3.</span> <span class="nav-text">5.1.3 Example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-nn-Dropout-p-0-5-inplace-False"><span class="nav-number">1.6.2.</span> <span class="nav-text">5.2 nn.Dropout(p=0.5, inplace=False)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-nn-ReLU-inplace-False"><span class="nav-number">1.6.3.</span> <span class="nav-text">5.3 nn.ReLU(inplace=False)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-nn-Embedding-num-embeddings-embedding-dim-padding-idx-None-max-norm-None-norm-type-2-scale-grad-by-freq-False-sparse-False"><span class="nav-number">1.6.4.</span> <span class="nav-text">5.4 nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-nn-Sequential-args"><span class="nav-number">1.6.5.</span> <span class="nav-text">5.5 nn.Sequential(*args)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-nn-CrossEntropyLoss"><span class="nav-number">1.6.6.</span> <span class="nav-text">5.6 nn.CrossEntropyLoss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-1-Parameters"><span class="nav-number">1.6.6.1.</span> <span class="nav-text">5.6.1 Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-2-Shape"><span class="nav-number">1.6.6.2.</span> <span class="nav-text">5.6.2 Shape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-3-Example"><span class="nav-number">1.6.6.3.</span> <span class="nav-text">5.6.3 Example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-7-nn-RNN"><span class="nav-number">1.6.7.</span> <span class="nav-text">5.7 nn.RNN()</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-1-Parameters"><span class="nav-number">1.6.7.1.</span> <span class="nav-text">5.7.1 Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-2-Shape"><span class="nav-number">1.6.7.2.</span> <span class="nav-text">5.7.2 Shape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-3-Example"><span class="nav-number">1.6.7.3.</span> <span class="nav-text">5.7.3 Example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-8-nn-LSTM"><span class="nav-number">1.6.8.</span> <span class="nav-text">5.8 nn.LSTM()</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-1-Parameters"><span class="nav-number">1.6.8.1.</span> <span class="nav-text">5.8.1 Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-2-Shape"><span class="nav-number">1.6.8.2.</span> <span class="nav-text">5.8.2 Shape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-8-3-Example"><span class="nav-number">1.6.8.3.</span> <span class="nav-text">5.8.3 Example</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-9-nn-GRU"><span class="nav-number">1.6.9.</span> <span class="nav-text">5.9 nn.GRU()</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-1-Parameters"><span class="nav-number">1.6.9.1.</span> <span class="nav-text">5.9.1 Parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-2-Shape"><span class="nav-number">1.6.9.2.</span> <span class="nav-text">5.9.2 Shape</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-9-3-Example"><span class="nav-number">1.6.9.3.</span> <span class="nav-text">5.9.3 Example</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-torch-optim"><span class="nav-number">1.7.</span> <span class="nav-text">6. torch.optim</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-如何使用-optimizer"><span class="nav-number">1.7.1.</span> <span class="nav-text">6.1 如何使用 optimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-常用函数"><span class="nav-number">1.7.2.</span> <span class="nav-text">6.2 常用函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-torch-utils"><span class="nav-number">1.8.</span> <span class="nav-text">7. torch.utils</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-torch-utils-data-Dataloader"><span class="nav-number">1.8.1.</span> <span class="nav-text">7.1 torch.utils.data.Dataloader</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-模型的保存与加载"><span class="nav-number">1.9.</span> <span class="nav-text">8. 模型的保存与加载</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-参考"><span class="nav-number">1.10.</span> <span class="nav-text">9. 参考</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2019 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wanncy</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        




  <script type="text/javascript">
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=66562237";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
